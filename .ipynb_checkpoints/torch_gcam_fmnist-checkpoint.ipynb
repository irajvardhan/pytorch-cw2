{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook has all the original code from Fooling nw intr. On top of that we have an added guided back prop module. However, subsequent calls to this class are failing throwing an error 'tried to call backward twice buffer already cleared, set retain_graph=True'.\n",
    "### Running the code without calling the class object is working normally.\n",
    "### An interesting observation is that the gcam output after being multiplied with img and the guided gradcam output are quite similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Function\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from keras import datasets as keras_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 1\n",
    "PATCH_SIZE = 14 # it is 64 for 224x224 image\n",
    "IMG_W = 28 # 224 originally\n",
    "IMG_H = 28 # 224 originally\n",
    "# MEANS = np.array([0.1307])\n",
    "# STDS = np.array([0.3081])\n",
    "MEANS = np.array([0.])\n",
    "STDS = np.array([1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, features, num_classes, init_weights=True):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4*4*50, 500),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(500, num_classes)\n",
    "        )\n",
    "        \n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        # x are the logits values\n",
    "        return x \n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for name, module in model._modules.items():\n",
    "#     print('name: ', name)\n",
    "#     print('module: ',module)\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Compute the per channel clamp_min and clamp_max respectively\n",
    "channel_clamp_min = (0 - MEANS) / STDS\n",
    "channel_clamp_max = (1 - MEANS) / STDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \"\"\" Class for extracting activations and\n",
    "    registering gradients from targeted intermediate layers \"\"\"\n",
    "    def __init__(self, model, target_layers):\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.gradients = []\n",
    "\n",
    "    def save_gradient(self, grad):\n",
    "        self.gradients.append(grad)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outputs = []\n",
    "        self.gradients = []\n",
    "        for name, module in self.model._modules.items():\n",
    "            x = module(x)\n",
    "            if name in self.target_layers:\n",
    "                # Store the features and register hook to save gradients for the target layer\n",
    "                x.register_hook(self.save_gradient)\n",
    "                outputs += [x]\n",
    "        return outputs, x\n",
    "\n",
    "\n",
    "class ModelOutputs:\n",
    "    \"\"\" Class for making a forward pass, and getting:\n",
    "    1. The network output.\n",
    "    2. Activations from intermediate targeted layers.\n",
    "    3. Gradients from intermediate targeted layers. \"\"\"\n",
    "    def __init__(self, model, target_layers):\n",
    "        self.model = model\n",
    "        self.feature_extractor = FeatureExtractor(self.model.features, target_layers)\n",
    "\n",
    "    def get_gradients(self):\n",
    "        # Retrieve the saved gradients for the target layer\n",
    "        return self.feature_extractor.gradients\n",
    "\n",
    "    def __call__(self, x):\n",
    "        target_activations, output = self.feature_extractor(x)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.model.classifier(output)\n",
    "        return target_activations, output\n",
    "\n",
    "\n",
    "def preprocess_image(input_image):\n",
    "    \"\"\"\n",
    "    This method normalizes the input image and converts it to a torch Variable\n",
    "    :param input_image: The input image to be pre-processed\n",
    "    :return: torch Variable of the normalized input image\n",
    "    \"\"\"\n",
    "    means = [0.485, 0.456, 0.406]\n",
    "    stds = [0.229, 0.224, 0.225]\n",
    "\n",
    "    normalized_image = input_image.copy()[:, :, ::-1]\n",
    "    for i in range(3):\n",
    "        normalized_image[:, :, i] = normalized_image[:, :, i] - means[i]\n",
    "        normalized_image[:, :, i] = normalized_image[:, :, i] / stds[i]\n",
    "    normalized_image = \\\n",
    "        np.ascontiguousarray(np.transpose(normalized_image, (2, 0, 1)))\n",
    "    normalized_image = torch.from_numpy(normalized_image)\n",
    "    normalized_image.unsqueeze_(0)\n",
    "    normalized_tensor = Variable(normalized_image, requires_grad=True).cuda()\n",
    "    return normalized_tensor\n",
    "\n",
    "# added by raj\n",
    "def preprocess_image_one_ch(input_image):\n",
    "    # use same values as in the transform.normalize method when the FMNIST model is trained\n",
    "    mean = MEANS[0]\n",
    "    std = STDS[0]\n",
    "    normalized_image = input_image.copy()\n",
    "    normalized_image = normalized_image.reshape(normalized_image.shape[0], normalized_image.shape[1], 1)\n",
    "    \n",
    "    normalized_image[:, :, 0] = normalized_image[:, :, 0] - mean\n",
    "    normalized_image[:, :, 0] = normalized_image[:, :, 0] / std\n",
    "    \n",
    "    normalized_image = \\\n",
    "        np.ascontiguousarray(np.transpose(normalized_image, (2, 0, 1)))\n",
    "    normalized_image = torch.from_numpy(normalized_image)\n",
    "    normalized_image.unsqueeze_(0)\n",
    "    normalized_tensor = Variable(normalized_image, requires_grad=True).cuda()\n",
    "    return normalized_tensor\n",
    "\n",
    "def show_cam_on_image(input_image, mask, filename=\"cam.png\"):\n",
    "    \"\"\"\n",
    "    Converts the mask to a heatmap and overlays it with the input image.\n",
    "    :param input_image: input image\n",
    "    :param mask: gradcam mask to be used as heatmap\n",
    "    :param filename: output path to write the image overlayed with the mask\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(input_image)\n",
    "    cam = cam / np.max(cam)\n",
    "    cv2.imwrite(filename, np.uint8(255 * cam))\n",
    "\n",
    "\n",
    "class GradCamAttack:\n",
    "    \"\"\"\n",
    "    This class is responsible for creating a targeted adversarial patch such that the top predicted category is the\n",
    "    target category and the Grad-CAM for the target category is hidden in the patch location.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_layer_names):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.model = model.cuda()\n",
    "        self.criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "        self.extractor = ModelOutputs(self.model, target_layer_names)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def __call__(self, image_tensor, index, target_class_index, lr=.005, eps=0.007, lambda_val=0.05, attack_iters=750):\n",
    "        print('\\n\\nOur adversarial patch attack:\\n\\n')\n",
    "        print('Before attack, Predicted class:{}\\tTarget class:{}\\n\\n'.format(index, target_class_index))\n",
    "\n",
    "        # Clone the original image for computing the adversarial image with the patch\n",
    "        adv_image_tensor = image_tensor.clone()\n",
    "\n",
    "        # Initialize the perturbation tensor\n",
    "        dl_dx_cumulative = torch.zeros_like(image_tensor)\n",
    "\n",
    "        # Specify the top-left co-ordinates and the size for the patch and create the corresponding mask\n",
    "        # The mask will have ones at the patch location pixels and zeros at all other pixels\n",
    "        start_pos = (0, 0)\n",
    "        #patch_size = 64\n",
    "        patch_size = PATCH_SIZE\n",
    "        mask = torch.zeros_like(image_tensor).cuda()\n",
    "        mask.data[0, :, start_pos[1]:start_pos[1] + patch_size, start_pos[0]:start_pos[0] + patch_size] = 1.0\n",
    "\n",
    "        # Means and std_devs used for pre-processing\n",
    "#         means = np.array([0.485, 0.456, 0.406])\n",
    "#         stds = np.array([0.229, 0.224, 0.225])\n",
    "        \n",
    "        # Compute the per channel clamp_min and clamp_max respectively\n",
    "        channel_clamp_min = (0 - MEANS) / STDS\n",
    "        channel_clamp_max = (1 - MEANS) / STDS\n",
    "\n",
    "        loss_zero_counter = 0\n",
    "        for i in range(attack_iters):\n",
    "            adv_features, adv_output = self.extractor(adv_image_tensor)\n",
    "            pred_index = np.argmax(adv_output.cpu().data.numpy())\n",
    "\n",
    "            # Create a one-hot tensor for the target category\n",
    "            one_hot = np.zeros((1, adv_output.size()[-1]), dtype=np.float32)\n",
    "            one_hot[0][target_class_index] = 1\n",
    "            one_hot = Variable(torch.from_numpy(one_hot), requires_grad=True)\n",
    "            one_hot_tensor = torch.sum(one_hot.cuda() * adv_output)\n",
    "\n",
    "            # Clear the gradients before loss computation\n",
    "            self.model.features.zero_grad()\n",
    "            self.model.classifier.zero_grad()\n",
    "\n",
    "            # Compute the gradients of the loss with respect to the feature layer for the adversarial image\n",
    "            dy_dz, = torch.autograd.grad(one_hot_tensor, adv_features[0],\n",
    "                                         grad_outputs=torch.ones(one_hot_tensor.size()).cuda(),\n",
    "                                         retain_graph=True, create_graph=True)\n",
    "            dy_dz_sum = dy_dz.sum(dim=2).sum(dim=2)\n",
    "\n",
    "            # Compute gradient weighted class activations for the perturbed image\n",
    "            grad_weighted_feats = dy_dz_sum.unsqueeze(-1).unsqueeze(-1) * adv_features[0]\n",
    "            gcam = grad_weighted_feats.sum(dim=1).squeeze(0)\n",
    "            gcam = self.relu(gcam)\n",
    "\n",
    "            # Normalize the gradcam tensor\n",
    "            gcam = gcam / (gcam.sum() + 1e-10)\n",
    "            \n",
    "            print('gcam shape = ', gcam.shape)\n",
    "\n",
    "            # Compute the loss for the patch location pixels in the gradcam tensor.\n",
    "            # For a 224x224 image, the adversarial patch size is 64x64.\n",
    "            # Since the gradcam tensor is 14x14 for VGG19 BN network, the corresponding gradcam patch size is 4x4\n",
    "            \n",
    "            # For a 28x28 image\n",
    "            \n",
    "            gcam_loss = torch.sum(gcam[0:4, 0:4]).abs().cuda()\n",
    "            gcam_loss = gcam_loss / 16.0\n",
    "\n",
    "            # Add the cross entropy loss if target category is not the top predicted category\n",
    "            if np.argmax(adv_output.cpu().data.numpy()) == target_class_index:\n",
    "                xe_loss = 0.0\n",
    "            else:\n",
    "                xe_loss = self.criterion(adv_output, torch.tensor([target_class_index], dtype=torch.long).cuda())\n",
    "\n",
    "            # We minimize both the gradcam loss and cross entropy loss\n",
    "            total_loss = gcam_loss + (lambda_val * xe_loss)\n",
    "\n",
    "            # Stop the attack once the loss is zero for 5 consecutive iterations\n",
    "            if total_loss == 0.0:\n",
    "                if loss_zero_counter > 5:\n",
    "                    break\n",
    "                else:\n",
    "                    loss_zero_counter += 1\n",
    "            else:\n",
    "                loss_zero_counter = 0\n",
    "\n",
    "            # Compute the gradient of the total loss with respect to the perturbed image\n",
    "            dl_dx, = torch.autograd.grad(total_loss, adv_image_tensor)\n",
    "\n",
    "            # Perform gradient ascent using the sign of dl_dx to compute the cumulative perturbation\n",
    "            dl_dx_cumulative = dl_dx_cumulative - lr * torch.sign(dl_dx)\n",
    "            adv_image_tensor = (1 - mask) * image_tensor.clone() + mask * dl_dx_cumulative\n",
    "\n",
    "            # Clamp the adversarial image using per channel min and max respectively\n",
    "            for c in range(NUM_CHANNELS):\n",
    "                adv_image_tensor[:, c, :, :] = adv_image_tensor[:, c, :, :].clamp(channel_clamp_min[c],\n",
    "                                                                                  channel_clamp_max[c])\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print('Iteration:{}\\tGradCAM Loss:{:.3f}\\tCE Loss:{:.3f}\\ttotal_pert.mean:{:.3f}\\tOrig index:{}'\n",
    "                      '\\tTarget index:{}\\tPred index:{}'.format(i, gcam_loss, xe_loss, dl_dx_cumulative.abs().mean(),\n",
    "                                                                index, target_class_index, pred_index))\n",
    "\n",
    "        # Store the resulting adversarial image tensor\n",
    "        res_adv_tensor = image_tensor.clone()\n",
    "        res_adv_tensor.data = adv_image_tensor.data\n",
    "\n",
    "        # Get the top predicted category of the resulting adversarial image tensor\n",
    "        _, adv_output = self.extractor(res_adv_tensor)\n",
    "\n",
    "        print('\\n\\nAfter attack, Original class: {}\\tPredicted class: {}\\tTarget class: {}'.\n",
    "              format(index, adv_output[0].argmax(), target_class_index))\n",
    "\n",
    "        # Denormalize the adversarial image\n",
    "        adv_img = res_adv_tensor.data[0].cpu().numpy()\n",
    "        adv_img = np.transpose(adv_img, (1, 2, 0))\n",
    "        for i in range(NUM_CHANNELS):\n",
    "            adv_img[:, :, i] = (adv_img[:, :, i] * STDS[i]) + MEANS[i]\n",
    "\n",
    "        return adv_img, res_adv_tensor\n",
    "\n",
    "\n",
    "class GradCamRegPatchAttack:\n",
    "    \"\"\"\n",
    "    This class is responsible for creating a regular adversarial patch for a targeted attack.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_layer_names):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.model = model.cuda()\n",
    "        self.criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "        self.extractor = ModelOutputs(self.model, target_layer_names)\n",
    "\n",
    "    def __call__(self, image_tensor, index, target_class_index, lr=.005, eps=0.007, lambda_val=0.05, attack_iters=750):\n",
    "        print('\\n\\nRegular adversarial patch attack:\\n\\n')\n",
    "        print('Before attack, Predicted class:{}\\tTarget class:{}\\n'.format(index, target_class_index))\n",
    "\n",
    "        # Clone the original image for computing the perturbed adversarial image\n",
    "        adv_image_tensor = image_tensor.clone() + torch.randn(image_tensor.size()).cuda() / 100\n",
    "\n",
    "        # Initialize the perturbation tensor\n",
    "        dl_dx_cumulative = torch.zeros_like(image_tensor)\n",
    "\n",
    "        # Means and std_devs used for pre-processing\n",
    "#         means = np.array([0.485, 0.456, 0.406])\n",
    "#         stds = np.array([0.229, 0.224, 0.225])\n",
    "        \n",
    "        # Compute the per channel clamp_min and clamp_max respectively\n",
    "        channel_clamp_min = (0 - MEANS) / STDS\n",
    "        channel_clamp_max = (1 - MEANS) / STDS\n",
    "\n",
    "        # Specify the top-left co-ordinates and the size for the patch and create the corresponding mask\n",
    "        # The mask will have ones at the patch location pixels and zeros at all other pixels\n",
    "        start_pos = (0, 0)\n",
    "        patch_size = PATCH_SIZE\n",
    "        mask = torch.zeros_like(image_tensor).cuda()\n",
    "        mask.data[0, :, start_pos[1]:start_pos[1] + patch_size, start_pos[0]:start_pos[0] + patch_size] = 1.0\n",
    "\n",
    "        loss_zero_counter = 0\n",
    "        target_flip_counter = 0\n",
    "        for i in range(attack_iters):\n",
    "            _, adv_output = self.extractor(adv_image_tensor)\n",
    "            pred_index = np.argmax(adv_output.cpu().data.numpy())\n",
    "\n",
    "            self.model.features.zero_grad()\n",
    "            self.model.classifier.zero_grad()\n",
    "\n",
    "            # Stop the attack once the target category is reached for 5 consecutive attack iterations\n",
    "            if i > 250 and pred_index == target_class_index:\n",
    "                if target_flip_counter > 5:\n",
    "                    break\n",
    "                else:\n",
    "                    target_flip_counter += 1\n",
    "            else:\n",
    "                target_flip_counter = 0\n",
    "\n",
    "            xe_loss = self.criterion(adv_output, torch.tensor([target_class_index], dtype=torch.long).cuda())\n",
    "\n",
    "            # Stop the attack once the loss is zero for 5 consecutive attack iterations\n",
    "            if xe_loss == 0.0:\n",
    "                if loss_zero_counter > 5:\n",
    "                    break\n",
    "                else:\n",
    "                    loss_zero_counter += 1\n",
    "            else:\n",
    "                loss_zero_counter = 0\n",
    "\n",
    "            # Compute the gradient of the total loss with respect to the perturbed image\n",
    "            dl_dx, = torch.autograd.grad(xe_loss, adv_image_tensor)\n",
    "\n",
    "            # Perform gradient ascent using the sign of dl_dx to compute the cumulative perturbation\n",
    "            dl_dx_cumulative = dl_dx_cumulative - lr * torch.sign(dl_dx)\n",
    "            adv_image_tensor = image_tensor.clone() * (1 - mask) + dl_dx_cumulative * mask\n",
    "\n",
    "            # Clamp the adversarial image using per channel min and max respectively\n",
    "            for c in range(NUM_CHANNELS):\n",
    "                adv_image_tensor[:, c, :, :] = adv_image_tensor[:, c, :, :].clamp(channel_clamp_min[c],\n",
    "                                                                                  channel_clamp_max[c])\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print('Iteration:{}\\tCE Loss:{:.3f}\\ttotal_pert.mean:{:.3f}\\tOrig index:{}\\tTarget index:{}'\n",
    "                      '\\tPred index:{}'.format(i, xe_loss, dl_dx_cumulative.abs().mean(),\n",
    "                                               index, target_class_index, pred_index))\n",
    "\n",
    "        # Store the resulting adversarial image tensor\n",
    "        res_adv_tensor = image_tensor.clone()\n",
    "        res_adv_tensor.data = adv_image_tensor.data\n",
    "\n",
    "        # Get the top predicted category of the resulting adversarial image tensor\n",
    "        _, adv_output = self.extractor(res_adv_tensor)\n",
    "\n",
    "        print('\\n\\nAfter attack, Original class: {}\\tPredicted class: {}\\tTarget class: {}'.\n",
    "              format(index, adv_output[0].argmax(), target_class_index))\n",
    "\n",
    "        # Denormalize the adversarial image\n",
    "        adv_img = res_adv_tensor.data[0].cpu().numpy()\n",
    "        adv_img = np.transpose(adv_img, (1, 2, 0))\n",
    "        for i in range(NUM_CHANNELS):\n",
    "            adv_img[:, :, i] = (adv_img[:, :, i] * STDS[i]) + MEANS[i]\n",
    "\n",
    "        return adv_img, res_adv_tensor\n",
    "\n",
    "\n",
    "class GradCam:\n",
    "    \"\"\"\n",
    "    This class computes the Grad-CAM mask for the specified index.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_layer_names):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.model = model.cuda()\n",
    "        self.extractor = ModelOutputs(self.model, target_layer_names)\n",
    "\n",
    "    def __call__(self, image_tensor, index=None):\n",
    "        features, output = self.extractor(image_tensor)\n",
    "\n",
    "        if index is None:\n",
    "            index = np.argmax(output.cpu().data.numpy())\n",
    "\n",
    "        # Compute the one-hot tensor corresponding to the index\n",
    "        one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
    "        one_hot[0][index] = 1\n",
    "        one_hot = Variable(torch.from_numpy(one_hot), requires_grad=True)\n",
    "        one_hot = torch.sum(one_hot.cuda() * output)\n",
    "\n",
    "        self.model.features.zero_grad()\n",
    "        self.model.classifier.zero_grad()\n",
    "        one_hot.backward(retain_graph=True)\n",
    "\n",
    "        # Get the gradients and features to compute Grad-CAM\n",
    "        grads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()\n",
    "        target = features[-1]\n",
    "        target = target.cpu().data.numpy()[0, :]\n",
    "\n",
    "        weights = np.mean(grads_val, axis=(2, 3))[0, :]\n",
    "        cam = np.zeros(target.shape[1:], dtype=np.float32)\n",
    "\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * target[i, :, :]\n",
    "\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = cv2.resize(cam, (IMG_W, IMG_W))\n",
    "        cam = cam - np.min(cam)\n",
    "        cam = cam / np.max(cam)\n",
    "        return cam\n",
    "\n",
    "class GuidedBackpropReLU(Function):\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        positive_mask = (input_tensor > 0).type_as(input_tensor)\n",
    "        output = torch.addcmul(torch.zeros(input_tensor.size()).type_as(input_tensor), input_tensor, positive_mask)\n",
    "        self.save_for_backward(input_tensor, output)\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        input_tensor, output = self.saved_tensors\n",
    "        grad_input = None\n",
    "\n",
    "        positive_mask_1 = (input_tensor > 0).type_as(grad_output)\n",
    "        positive_mask_2 = (grad_output > 0).type_as(grad_output)\n",
    "        grad_input = torch.addcmul(torch.zeros(input_tensor.size()).\n",
    "                                   type_as(input_tensor), torch.addcmul(torch.zeros(input_tensor.size()).\n",
    "                                        type_as(input_tensor), grad_output, positive_mask_1), positive_mask_2)\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "class GuidedBackpropReLUModel:\n",
    "    def __init__(self, model, use_cuda):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.cuda = use_cuda\n",
    "        if self.cuda:\n",
    "            self.model = model.cuda()\n",
    "\n",
    "        # replace ReLU with GuidedBackpropReLU\n",
    "        for idx, module in self.model.features._modules.items():\n",
    "            if module.__class__.__name__ == 'ReLU':\n",
    "                self.model.features._modules[idx] = GuidedBackpropReLU()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        return self.model(input_tensor)\n",
    "\n",
    "    def __call__(self, input_tensor, index = None):\n",
    "        if self.cuda:\n",
    "            output = self.forward(input_tensor.cuda())\n",
    "        else:\n",
    "            output = self.forward(input_tensor)\n",
    "\n",
    "        if index == None:\n",
    "            index = np.argmax(output.cpu().data.numpy())\n",
    "\n",
    "        one_hot = np.zeros((1, output.size()[-1]), dtype = np.float32)\n",
    "        one_hot[0][index] = 1\n",
    "        one_hot = Variable(torch.from_numpy(one_hot), requires_grad = True)\n",
    "        if self.cuda:\n",
    "            one_hot = torch.sum(one_hot.cuda() * output)\n",
    "        else:\n",
    "            one_hot = torch.sum(one_hot * output)\n",
    "\n",
    "        # self.model.features.zero_grad()\n",
    "        # self.model.classifier.zero_grad()\n",
    "        \n",
    "        #one_hot.backward(retain_variables=True)\n",
    "        one_hot.backward(retain_graph=True)\n",
    "\n",
    "        output = input_tensor.grad.cpu().data.numpy()\n",
    "        output = output[0,:,:,:]\n",
    "\n",
    "        return output\n",
    "    \n",
    "class GuidedGradCam:\n",
    "    \"\"\"\n",
    "    This class computes the Guided Grad-CAM mask for the specified index.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_layer_names):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.model = model.cuda()\n",
    "        self.extractor = ModelOutputs(self.model, target_layer_names)\n",
    "\n",
    "    def __call__(self, image_tensor, index=None):\n",
    "        features, output = self.extractor(image_tensor)\n",
    "\n",
    "        if index is None:\n",
    "            index = np.argmax(output.cpu().data.numpy())\n",
    "\n",
    "        # Means and std_devs used for pre-processing\n",
    "#         means = np.array([0.485, 0.456, 0.406])\n",
    "#         stds = np.array([0.229, 0.224, 0.225])\n",
    "        \n",
    "        # Compute the one-hot tensor corresponding to the index\n",
    "        one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
    "        one_hot[0][index] = 1\n",
    "        one_hot = Variable(torch.from_numpy(one_hot), requires_grad=True)\n",
    "        one_hot = torch.sum(one_hot.cuda() * output)\n",
    "\n",
    "        self.model.features.zero_grad()\n",
    "        self.model.classifier.zero_grad()\n",
    "        one_hot.backward(retain_graph=True)\n",
    "\n",
    "        # Get the gradients and features to compute Grad-CAM\n",
    "        grads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()\n",
    "        target = features[-1]\n",
    "        target = target.cpu().data.numpy()[0, :]\n",
    "\n",
    "        weights = np.mean(grads_val, axis=(2, 3))[0, :]\n",
    "        cam = np.zeros(target.shape[1:], dtype=np.float32)\n",
    "\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * target[i, :, :]\n",
    "\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = cv2.resize(cam, (IMG_W, IMG_H))\n",
    "        cam = cam - np.min(cam)\n",
    "        cam = cam / np.max(cam)\n",
    "        \n",
    "        # Gradcam ends here. Below additional code is added to produce guided grad-cam\n",
    "        img = image_tensor.data[0].cpu().numpy()\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "#         for i in range(3):\n",
    "#             img[:, :, i] = (img[:, :, i] * stds[i]) + means[i]\n",
    "        cam_heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
    "        cam_heatmap = cv2.cvtColor(cam_heatmap, cv2.COLOR_BGR2RGB)\n",
    "        cam = np.float32(cam.reshape((28, 28, 1))) * np.float32(img)\n",
    "        cam = 255 * cam / np.max(cam)\n",
    "        cam = np.uint8(cam)\n",
    "        \n",
    "        gb_viz -= np.min(gb_viz)\n",
    "        gb_viz /= gb_viz.max()\n",
    "        img_int = (gb_viz * 255.).astype(int).reshape(img.shape[:2])\n",
    "\n",
    "        gd_gb = gb_viz * cam\n",
    "        img_int = (gd_gb * 255.).astype(int).reshape(img.shape[:2])\n",
    "\n",
    "        img_int = img_int/float(np.amax(img_int))\n",
    "\n",
    "        return cam\n",
    "\n",
    "\n",
    "def forward_inference(model, input_tensor):\n",
    "    \"\"\"\n",
    "    Computes forward inference on the input image tensor and\n",
    "    returns the top prediction index and probability\n",
    "    :param model:\n",
    "    :param input_tensor:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    output = model(input_tensor)\n",
    "    index = np.argmax(output.cpu().data.numpy())\n",
    "    index_prob = torch.nn.functional.softmax(output)[0][index]\n",
    "    return index, index_prob\n",
    "\n",
    "\n",
    "def get_args():\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available. Exiting.\")\n",
    "        exit()\n",
    "    print(\"Using GPU for acceleration\")\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--image-path', type=str, default='./examples/ILSVRC2012_val_00008855.JPEG',\n",
    "                        help='Input image path')\n",
    "    parser.add_argument('--result-dir', type=str, default='./results', help='Path to store the results')\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taken from http://localhost:8890/notebooks/pytorch-cw2/torch_fmnist_train_v2.ipynb\n",
    "\"\"\"\n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, \n",
    "stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "\"\"\"\n",
    "\n",
    "def make_layers(cfg, in_channels, kernel_size, stride, padding, batch_norm=False):\n",
    "    layers = []\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=kernel_size, padding=padding)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Refer VGG19_bn configurationh here: \n",
    "https://github.com/pytorch/vision/blob/76702a03d6cc2e4f431bfd1914d5e301c07bd489/torchvision/models/vgg.py#L63\n",
    "\"\"\"\n",
    "cfgs = {\n",
    "    #'E': [64, 64, 'M',128, 128, 'M',256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M',512, 512, 512, 512, 'M'],\n",
    "    'E': [20, 'M', 50, 'M']\n",
    "}\n",
    "\n",
    "model_layers = make_layers(cfgs['E'],in_channels=1, kernel_size=5, stride=1, padding=0, batch_norm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' python gradcam_targeted_patch_attack.py --image-path <path_to_image> --result-dir <path_to_result_dir>\\n1. Loads an image with opencv.\\n2. Preprocesses it for VGG19 and converts to a pytorch variable.\\n3. Makes a forward pass to find the category index with the highest score,\\nand computes intermediate activations.\\nMakes the visualization. '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" python gradcam_targeted_patch_attack.py --image-path <path_to_image> --result-dir <path_to_result_dir>\n",
    "1. Loads an image with opencv.\n",
    "2. Preprocesses it for VGG19 and converts to a pytorch variable.\n",
    "3. Makes a forward pass to find the category index with the highest score,\n",
    "and computes intermediate activations.\n",
    "Makes the visualization. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# args = get_args()\n",
    "image_path = './examples/ILSVRC2012_val_00008855.JPEG' #Input image path\n",
    "result_dir = './results' #Path to store the results\n",
    "\n",
    "use_cuda = True\n",
    "\n",
    "# Setting the seed for reproducibility for demo\n",
    "# Comment the below 4 lines for the target category to be random across runs\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=800, out_features=500, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=500, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = 10\n",
    "\n",
    "pretrained_model = \"model/fmnist/v2/fmnist_cnn.pt\"\n",
    "# Initialize the network\n",
    "model = Net(model_layers, num_classes=NUM_CLASSES).cuda()\n",
    "\n",
    "# Load the pretrained model\n",
    "model.load_state_dict(torch.load(pretrained_model))\n",
    "\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Can work with any model, but it assumes that the model has a feature method,\n",
    "# and a classifier method, as in the VGG models in torchvision.\n",
    "gradcam_attack = GradCamAttack(model, target_layer_names=[\"4\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gradcam_reg_patch_attack = GradCamRegPatchAttack(model=models.vgg19_bn(pretrained=True), target_layer_names=[\"51\"])\n",
    "gradcam_reg_patch_attack = GradCamRegPatchAttack(model, target_layer_names=[\"4\"])\n",
    "#gradcam = GradCam(model=models.vgg19_bn(pretrained=True), target_layer_names=[\"51\"])\n",
    "gradcam = GradCam(model, target_layer_names=[\"4\"])\n",
    "\n",
    "#pretrained_vgg_net = models.vgg19_bn(pretrained=True).cuda()\n",
    "\n",
    "\n",
    "#pretrained_vgg_net = pretrained_vgg_net.eval()\n",
    "\n",
    "#image_name = image_path.split('/')[-1].split('.')[0]\n",
    "#print('image_name ',image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create result directory if it doesn't exist\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "# Read the input image and preprocess to a tensor\n",
    "# img = cv2.imread(image_path, 1)\n",
    "# img = np.float32(cv2.resize(img, (224, 224))) / 255\n",
    "(x_train, y_train) ,(x_test, y_test) = keras_datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_index = 18\n",
    "img_sample = x_train[chosen_index]/255\n",
    "y_train[chosen_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7feaab8c64e0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[chosen_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "#preprocessed_img = preprocess_image(img)\n",
    "preprocessed_img = preprocess_image_one_ch(img_sample).float()\n",
    "print(preprocessed_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original index: 6 and original probability: 0.5639716982841492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/user/raj_vardhan/.conda/envs/tf-gpu-env/lib/python3.6/site-packages/ipykernel_launcher.py:517: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# Get the original prediction index and the corresponding probability\n",
    "orig_index, orig_prob = forward_inference(model, preprocessed_img)\n",
    "print('Original index: {} and original probability: {}'.format(orig_index, orig_prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target index is  5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Pick a random target from the remaining NUM_CLASSES-1 categories excluding the original prediction\n",
    "list_of_idx = np.delete(np.arange(NUM_CLASSES), orig_index)\n",
    "rand_idx = np.random.randint(NUM_CLASSES-1)\n",
    "target_index = list_of_idx[rand_idx]\n",
    "print('target index is ', target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Regular adversarial patch attack:\n",
      "\n",
      "\n",
      "Before attack, Predicted class:6\tTarget class:5\n",
      "\n",
      "Iteration:0\tCE Loss:4.224\ttotal_pert.mean:0.005\tOrig index:6\tTarget index:5\tPred index:6\n",
      "Iteration:10\tCE Loss:2.066\ttotal_pert.mean:0.049\tOrig index:6\tTarget index:5\tPred index:8\n",
      "Iteration:20\tCE Loss:1.969\ttotal_pert.mean:0.095\tOrig index:6\tTarget index:5\tPred index:8\n",
      "Iteration:30\tCE Loss:1.882\ttotal_pert.mean:0.142\tOrig index:6\tTarget index:5\tPred index:6\n",
      "Iteration:40\tCE Loss:1.809\ttotal_pert.mean:0.186\tOrig index:6\tTarget index:5\tPred index:5\n",
      "Iteration:50\tCE Loss:1.759\ttotal_pert.mean:0.228\tOrig index:6\tTarget index:5\tPred index:5\n",
      "Iteration:60\tCE Loss:1.728\ttotal_pert.mean:0.269\tOrig index:6\tTarget index:5\tPred index:5\n",
      "Iteration:70\tCE Loss:1.702\ttotal_pert.mean:0.308\tOrig index:6\tTarget index:5\tPred index:5\n",
      "Iteration:80\tCE Loss:1.685\ttotal_pert.mean:0.348\tOrig index:6\tTarget index:5\tPred index:5\n",
      "Iteration:90\tCE Loss:1.669\ttotal_pert.mean:0.389\tOrig index:6\tTarget index:5\tPred index:5\n",
      "Iteration:100\tCE Loss:1.650\ttotal_pert.mean:0.431\tOrig index:6\tTarget index:5\tPred index:5\n",
      "Iteration:110\tCE Loss:1.634\ttotal_pert.mean:0.473\tOrig index:6\tTarget index:5\tPred index:5\n",
      "Iteration:120\tCE Loss:1.623\ttotal_pert.mean:0.514\tOrig index:6\tTarget index:5\tPred index:5\n",
      "Iteration:130\tCE Loss:1.610\ttotal_pert.mean:0.554\tOrig index:6\tTarget index:5\tPred index:5\n",
      "Iteration:140\tCE Loss:1.598\ttotal_pert.mean:0.594\tOrig index:6\tTarget index:5\tPred index:5\n",
      "Iteration:150\tCE Loss:1.587\ttotal_pert.mean:0.635\tOrig index:6\tTarget index:5\tPred index:5\n",
      "Iteration:160\tCE Loss:1.577\ttotal_pert.mean:0.675\tOrig index:6\tTarget index:5\tPred index:5\n",
      "Iteration:170\tCE Loss:1.569\ttotal_pert.mean:0.716\tOrig index:6\tTarget index:5\tPred index:5\n",
      "Iteration:180\tCE Loss:1.557\ttotal_pert.mean:0.756\tOrig index:6\tTarget index:5\tPred index:5\n",
      "Iteration:190\tCE Loss:1.544\ttotal_pert.mean:0.795\tOrig index:6\tTarget index:5\tPred index:5\n",
      "Iteration:200\tCE Loss:1.531\ttotal_pert.mean:0.835\tOrig index:6\tTarget index:5\tPred index:5\n",
      "Iteration:210\tCE Loss:1.519\ttotal_pert.mean:0.873\tOrig index:6\tTarget index:5\tPred index:5\n",
      "Iteration:220\tCE Loss:1.513\ttotal_pert.mean:0.912\tOrig index:6\tTarget index:5\tPred index:5\n",
      "Iteration:230\tCE Loss:1.508\ttotal_pert.mean:0.951\tOrig index:6\tTarget index:5\tPred index:5\n",
      "Iteration:240\tCE Loss:1.504\ttotal_pert.mean:0.990\tOrig index:6\tTarget index:5\tPred index:5\n",
      "Iteration:250\tCE Loss:1.502\ttotal_pert.mean:1.030\tOrig index:6\tTarget index:5\tPred index:5\n",
      "\n",
      "\n",
      "After attack, Original class: 6\tPredicted class: 5\tTarget class: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/user/raj_vardhan/.conda/envs/tf-gpu-env/lib/python3.6/site-packages/ipykernel_launcher.py:517: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the regular adv patch attack image and the corresponding GradCAM\n",
    "reg_patch_adv_img, reg_patch_adv_tensor = gradcam_reg_patch_attack(preprocessed_img, orig_index, target_index)\n",
    "reg_patch_pred_index, reg_patch_pred_prob = forward_inference(model,preprocess_image_one_ch(reg_patch_adv_img[:, :, ::-1]))\n",
    "\n",
    "image_name = \"my_img\"\n",
    "cv2.imwrite(os.path.join(result_dir, image_name + '_reg_adv_patch_image.png'),\n",
    "            np.uint8(255 * np.clip(reg_patch_adv_img[:, :, ::-1], 0, 1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(reg_patch_adv_img.reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the GradCAM heatmap for the target category using the regular patch adversarial image\n",
    "reg_patch_adv_mask = gradcam(reg_patch_adv_tensor, target_index)\n",
    "show_cam_on_image(np.clip(reg_patch_adv_img[:, :, ::-1], 0, 1), reg_patch_adv_mask,\n",
    "                  filename=os.path.join(result_dir, image_name + '_reg_adv_patch_gcam.JPEG'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(reg_patch_adv_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Compute the adv patch attack using our method and the corresponding GradCAM\n",
    "# our_patch_adv_tensor has shape: torch.Size([1, 3, 224, 224])\n",
    "# our_patch_adv_img has shape: (224, 224, 3)\n",
    "our_patch_adv_img, our_patch_adv_tensor = gradcam_attack(preprocessed_img, orig_index, target_index)\n",
    "our_patch_pred_index, our_patch_pred_prob = forward_inference(model,\n",
    "                                                              preprocess_image_one_ch(our_patch_adv_img[:, :, ::-1]))\n",
    "cv2.imwrite(os.path.join(result_dir, image_name + '_our_adv_patch_image.png'),\n",
    "            np.uint8(255 * np.clip(our_patch_adv_img[:, :, ::-1], 0, 1)))\n",
    "\n",
    "# Generate the GradCAM heatmap for the target category using our patch adversarial image\n",
    "mask_adv_ = gradcam(our_patch_adv_tensor, target_index)\n",
    "show_cam_on_image(np.clip(our_patch_adv_img[:, :, ::-1], 0, 1), mask_adv_,\n",
    "                  filename=os.path.join(result_dir, image_name + '_our_adv_patch_gcam.JPEG'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the GradCAM heatmap for the target category using our patch adversarial image\n",
    "mask_adv_ = gradcam(our_patch_adv_tensor, target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cam = mask_adv_\n",
    "plt.imshow(cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradcam ends here. Below additional code is added to produce guided grad-cam\n",
    "\n",
    "image_tensor = our_patch_adv_tensor\n",
    "\n",
    "img = image_tensor.data[0].cpu().numpy()\n",
    "img = np.transpose(img, (1, 2, 0))\n",
    "#         for i in range(3):\n",
    "#             img[:, :, i] = (img[:, :, i] * stds[i]) + means[i]\n",
    "cam_heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
    "plt.imshow(cam_heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cam_heatmap = cv2.cvtColor(cam_heatmap, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(cam_heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(cam.shape)\n",
    "print(img.shape)\n",
    "\n",
    "cam = np.float32(cam.reshape((28, 28, 1))) * np.float32(img)\n",
    "cam = 255 * cam / np.max(cam)\n",
    "cam = np.uint8(cam)\n",
    "\n",
    "# gb_viz -= np.min(gb_viz)\n",
    "# gb_viz /= gb_viz.max()\n",
    "# img_int = (gb_viz * 255.).astype(int).reshape(img.shape[:2])\n",
    "\n",
    "# gd_gb = gb_viz * cam\n",
    "# img_int = (gd_gb * 255.).astype(int).reshape(img.shape[:2])\n",
    "\n",
    "# img_int = img_int/float(np.amax(img_int))\n",
    "\n",
    "print(preprocessed_img.shape)\n",
    "print(our_patch_adv_img.shape)\n",
    "print(our_patch_adv_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the GradCAM heatmap for the original class\n",
    "mask_orig = gradcam(preprocessed_img, orig_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mask_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7feaab857a90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cam = mask_orig\n",
    "plt.imshow(cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_tensor = preprocessed_img\n",
    "\n",
    "img = image_tensor.data[0].cpu().numpy()\n",
    "img = np.transpose(img, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img -= np.min(img)\n",
    "img /= img.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n",
      "(28, 28)\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)\n",
    "print(cam.shape)\n",
    "\n",
    "print(np.max(cam))\n",
    "print(np.min(cam))\n",
    "\n",
    "print(np.max(img))\n",
    "print(np.min(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cam = np.float32(cam.reshape((28, 28, 1))) * np.float32(img)\n",
    "cam = 255 * cam / np.max(cam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cam[0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cam = np.uint8(cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7feaa972f780>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(cam.reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# input_gb = Variable(preprocessed_img, requires_grad = True)\n",
    "\n",
    "# gb_model = GuidedBackpropReLUModel(model, use_cuda=use_cuda)\n",
    "# gb = gb_model(input_gb, orig_index)\n",
    "\n",
    "# print(gb.shape)\n",
    "# gb_tr = np.transpose(gb, (1,2,0))\n",
    "# plt.imshow(gb_tr.reshape(28,28), cmap='gray')\n",
    "\n",
    "# mean = 0.1307\n",
    "# std = 0.3081\n",
    "# gb_scaled = (gb_tr + mean)* std\n",
    "\n",
    "# plt.imshow(gb_scaled.reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input_gb = Variable(reg_patch_adv_tensor, requires_grad = True)\n",
    "\n",
    "# gb_model = GuidedBackpropReLUModel(model, use_cuda=use_cuda)\n",
    "# gb = gb_model(input_gb, orig_index)\n",
    "\n",
    "# print(gb.shape)\n",
    "# gb_tr = np.transpose(gb, (1,2,0))\n",
    "# plt.imshow(gb_tr.reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gb = gb_model(input_gb, orig_index)\n",
    "\n",
    "# print(gb.shape)\n",
    "# gb_tr = np.transpose(gb, (1,2,0))\n",
    "# plt.imshow(gb_tr.reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input_gb = Variable(reg_patch_adv_tensor, requires_grad = True)\n",
    "\n",
    "# gb_model = GuidedBackpropReLUModel(model, use_cuda=use_cuda)\n",
    "# gb = gb_model(input_gb, orig_index)\n",
    "\n",
    "# print(gb.shape)\n",
    "# gb_tr = np.transpose(gb, (1,2,0))\n",
    "# plt.imshow(gb_tr.reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_gb = Variable(preprocessed_img, requires_grad = True)\n",
    "output = model(input_gb)\n",
    "index = target_index\n",
    "one_hot = np.zeros((1, output.size()[-1]), dtype = np.float32)\n",
    "one_hot[0][index] = 1\n",
    "one_hot = Variable(torch.from_numpy(one_hot), requires_grad = True)\n",
    "one_hot = torch.sum(one_hot.cuda() * output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.2433, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot.backward(retain_graph=True)\n",
    "output = input_gb.grad.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 28, 28)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = output[0,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7feaa970af28>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(output[0],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb_viz = np.transpose(output, (1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n",
      "(28, 28, 1)\n",
      "-0.3449068\n",
      "0.35000515\n",
      "0\n",
      "255\n"
     ]
    }
   ],
   "source": [
    "print(gb_viz.shape)\n",
    "print(cam.shape)\n",
    "print(np.min(gb_viz))\n",
    "print(np.max(gb_viz))\n",
    "\n",
    "print(np.min(cam))\n",
    "print(np.max(cam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb_viz -= np.min(gb_viz)\n",
    "gb_viz /= np.max(gb_viz)\n",
    "gd_gb = gb_viz * cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "129.86676\n"
     ]
    }
   ],
   "source": [
    "print(np.min(gd_gb))\n",
    "print(np.max(gd_gb)) # will be between 0 and 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "img_int = (gd_gb * 255.).astype(int).reshape(img.shape[:2])\n",
    "img_int = img_int/float(np.amax(img_int))\n",
    "print(img_int.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7feaa9666a20>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
