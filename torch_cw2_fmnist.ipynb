{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are normalizing from 0 to 1 without subtracting mean and dividing by std unlike other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torch.utils import data as D\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilons = [0, .05, .1, .15, .2, .25, .3]\n",
    "use_cuda=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, features, num_classes, init_weights=True):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4*4*50, 500),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(500, num_classes)\n",
    "        )\n",
    "        \n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        # x are the logits values\n",
    "        return x \n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, \n",
    "stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "\"\"\"\n",
    "\n",
    "def make_layers(cfg, in_channels, kernel_size, stride, padding, batch_norm=False):\n",
    "    layers = []\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=kernel_size, padding=padding)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Refer VGG19_bn configurationh here: \n",
    "https://github.com/pytorch/vision/blob/76702a03d6cc2e4f431bfd1914d5e301c07bd489/torchvision/models/vgg.py#L63\n",
    "\"\"\"\n",
    "cfgs = {\n",
    "    #'E': [64, 64, 'M',128, 128, 'M',256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M',512, 512, 512, 512, 'M'],\n",
    "    'E': [20, 'M', 50, 'M']\n",
    "}\n",
    "\n",
    "model_layers = make_layers(cfgs['E'],in_channels=1, kernel_size=5, stride=1, padding=0, batch_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomDS(D.Dataset):\n",
    "    \"\"\"\n",
    "    A customized data loader.\n",
    "    \"\"\"\n",
    "    def __init__(self, path, train=True):\n",
    "        \"\"\" Intialize the dataset\n",
    "        \"\"\"\n",
    "        if train:\n",
    "            data_path = os.path.join(path,'x_train.npy')\n",
    "            targets_path = os.path.join(path,'y_train.npy')\n",
    "        else:\n",
    "            data_path = os.path.join(path,'x_test.npy')\n",
    "            targets_path = os.path.join(path,'y_test.npy')\n",
    "\n",
    "        self.path = data_path\n",
    "        self.data = np.load(data_path)\n",
    "        self.targets = np.load(targets_path)\n",
    "        #self.transform = transforms.ToTensor()\n",
    "        self.transform = transforms.Compose([\n",
    "                       transforms.ToTensor()\n",
    "                       #transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])\n",
    "        self.len = np.shape(self.data)[0]\n",
    "        \n",
    "    # You must override __getitem__ and __len__\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset\n",
    "        \"\"\"\n",
    "        data = self.data[index]\n",
    "        image = Image.fromarray(data)\n",
    "        \n",
    "        target = int(self.targets[index])\n",
    "        \n",
    "        #data = (data * 255).astype(np.uint8)\n",
    "        #data = data.reshape(28,28)\n",
    "        #image = Image.fromarray((data * 255).astype(np.uint8))\n",
    "        #image = Image.fromarray(data.astype(np.uint8))\n",
    "        \n",
    "        return self.transform(image), target\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape : (60000, 28, 28)\n",
      "y_train shape : (60000,)\n",
      "x_test shape : (10000, 28, 28)\n",
      "y_test shape : (10000,)\n",
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "directory = './data/fmnist'\n",
    "\n",
    "IS_DATA_READY = True\n",
    "assert(IS_DATA_READY == True)\n",
    "\n",
    "x_train = np.load(directory + '/x_train.npy')\n",
    "y_train = np.load(directory + '/y_train.npy')\n",
    "x_test = np.load(directory + '/x_test.npy')\n",
    "y_test = np.load(directory + '/y_test.npy')\n",
    "print('x_train shape : {}'.format(x_train.shape))\n",
    "print('y_train shape : {}'.format(y_train.shape))\n",
    "print('x_test shape : {}'.format(x_test.shape))\n",
    "print('y_test shape : {}'.format(y_test.shape))\n",
    "\n",
    "\n",
    "# Simple dataset. Only save path to image and load it and transform to tensor when call __getitem__.\n",
    "filepath = './data/fmnist/'\n",
    "train_set = CustomDS(filepath, train=True)\n",
    "test_set = CustomDS(filepath, train=False)\n",
    "\n",
    "# total images in set\n",
    "print(train_set.len)\n",
    "print(test_set.len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# main method\n",
    "## Training settings\n",
    "# input batch size for training (default: 64)\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# input batch size for testing (default: 1000)\n",
    "TEST_BATCH_SIZE = 1\n",
    "\n",
    "# number of epochs to train\n",
    "EPOCHS = 10\n",
    "\n",
    "#learning rate (default: 0.01)\n",
    "LR = 0.01\n",
    "\n",
    "#SGD momentum (default: 0.5)\n",
    "MOMENTUM = 0.5\n",
    "\n",
    "# how many batches to wait before logging training status\n",
    "LOG_INTERVAL = 10\n",
    "\n",
    "SAVE_MODEL = True\n",
    "SEED = 1\n",
    "NO_CUDA = False\n",
    "USE_CUDA = not NO_CUDA and torch.cuda.is_available()\n",
    "\n",
    "NUM_CLASSES=10\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if USE_CUDA else {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=800, out_features=500, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=500, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FMNIST Test dataset and dataloader declaration\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST('./data/', train=False, download=True, \n",
    "#                    transform=transforms.Compose([transforms.ToTensor(),])),\n",
    "#                    batch_size=1, shuffle=True)\n",
    "\n",
    "train_loader = D.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
    "test_loader = D.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
    "\n",
    "\n",
    "# Define what device we are using\n",
    "print(\"CUDA Available: \",torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# # Initialize the network\n",
    "# model = Net().to(device)\n",
    "\n",
    "# # Load the pretrained model\n",
    "# model.load_state_dict(torch.load(pretrained_model, map_location='cpu'))\n",
    "\n",
    "# # Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "# model.eval()\n",
    "\n",
    "pretrained_model = \"model/fmnist/v2/fmnist_cnn.pt\"\n",
    "# Initialize the network\n",
    "model = Net(model_layers, num_classes=NUM_CLASSES).cuda()\n",
    "\n",
    "# Load the pretrained model\n",
    "model.load_state_dict(torch.load(pretrained_model))\n",
    "\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import operator as op\n",
    "import functools as ft\n",
    "\n",
    "\n",
    "'''reduce_* helper functions reduce tensors on all dimensions but the first.\n",
    "They are intended to be used on batched tensors where dim 0 is the batch dim.\n",
    "'''\n",
    "\n",
    "\n",
    "def reduce_sum(x, keepdim=True):\n",
    "    # silly PyTorch, when will you get proper reducing sums/means?\n",
    "    for a in reversed(range(1, x.dim())):\n",
    "        x = x.sum(a, keepdim=keepdim)\n",
    "    return x\n",
    "\n",
    "\n",
    "def reduce_mean(x, keepdim=True):\n",
    "    numel = ft.reduce(op.mul, x.size()[1:])\n",
    "    x = reduce_sum(x, keepdim=keepdim)\n",
    "    return x / numel\n",
    "\n",
    "\n",
    "def reduce_min(x, keepdim=True):\n",
    "    for a in reversed(range(1, x.dim())):\n",
    "        x = x.min(a, keepdim=keepdim)[0]\n",
    "    return x\n",
    "\n",
    "\n",
    "def reduce_max(x, keepdim=True):\n",
    "    for a in reversed(range(1, x.dim())):\n",
    "        x = x.max(a, keepdim=keepdim)[0]\n",
    "    return x\n",
    "\n",
    "\n",
    "def torch_arctanh(x, eps=1e-6):\n",
    "    x *= (1. - eps)\n",
    "    return (torch.log((1 + x) / (1 - x))) * 0.5\n",
    "\n",
    "\n",
    "def l2r_dist(x, y, keepdim=True, eps=1e-8):\n",
    "    d = (x - y)**2\n",
    "    d = reduce_sum(d, keepdim=keepdim)\n",
    "    d += eps  # to prevent infinite gradient at 0\n",
    "    return d.sqrt()\n",
    "\n",
    "\n",
    "def l2_dist(x, y, keepdim=True):\n",
    "    d = (x - y)**2\n",
    "    return reduce_sum(d, keepdim=keepdim)\n",
    "\n",
    "\n",
    "def l1_dist(x, y, keepdim=True):\n",
    "    d = torch.abs(x - y)\n",
    "    return reduce_sum(d, keepdim=keepdim)\n",
    "\n",
    "\n",
    "def l2_norm(x, keepdim=True):\n",
    "    norm = reduce_sum(x*x, keepdim=keepdim)\n",
    "    return norm.sqrt()\n",
    "\n",
    "\n",
    "def l1_norm(x, keepdim=True):\n",
    "    return reduce_sum(x.abs(), keepdim=keepdim)\n",
    "\n",
    "\n",
    "def rescale(x, x_min=-1., x_max=1.):\n",
    "    return x * (x_max - x_min) + x_min\n",
    "\n",
    "\n",
    "def tanh_rescale(x, x_min=-1., x_max=1.):\n",
    "    return (torch.tanh(x) + 1) * 0.5 * (x_max - x_min) + x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"PyTorch Carlini and Wagner L2 attack algorithm.\n",
    "\n",
    "Based on paper by Carlini & Wagner, https://arxiv.org/abs/1608.04644 and a reference implementation at\n",
    "https://github.com/tensorflow/cleverhans/blob/master/cleverhans/attacks_tf.py\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from torch import autograd\n",
    "#from .helpers import *\n",
    "\n",
    "\n",
    "class AttackCarliniWagnerL2:\n",
    "\n",
    "    def __init__(self, targeted=True, search_steps=None, max_steps=None, cuda=True, debug=False):\n",
    "        self.debug = debug\n",
    "        self.targeted = targeted\n",
    "        #self.num_classes = 1000\n",
    "        self.num_classes = 10\n",
    "        self.confidence = 0  # FIXME need to find a good value for this, 0 value used in paper not doing much...\n",
    "        self.initial_const = 0.1  # bumped up from default of .01 in reference code\n",
    "        self.binary_search_steps = search_steps or 5\n",
    "        self.repeat = self.binary_search_steps >= 10\n",
    "        self.max_steps = max_steps or 1000\n",
    "        self.abort_early = True\n",
    "        self.clip_min = -1.\n",
    "        self.clip_max = 1.\n",
    "        self.cuda = cuda\n",
    "        self.clamp_fn = 'tanh'  # set to something else perform a simple clamp instead of tanh\n",
    "        self.init_rand = False  # an experiment, does a random starting point help?\n",
    "\n",
    "    def _compare(self, output, target):\n",
    "        if not isinstance(output, (float, int, np.int64)):\n",
    "            output = np.copy(output)\n",
    "            if self.targeted:\n",
    "                output[target] -= self.confidence\n",
    "            else:\n",
    "                output[target] += self.confidence\n",
    "            output = np.argmax(output)\n",
    "        if self.targeted:\n",
    "            return output == target\n",
    "        else:\n",
    "            return output != target\n",
    "\n",
    "    def _loss(self, output, target, dist, scale_const):\n",
    "        # compute the probability of the label class versus the maximum other\n",
    "        \n",
    "        # for the targeted attack, real will contain the current logit values for the targeted class\n",
    "        # This basically tell us what is the current probability of the image being classified as the target class\n",
    "        # multiplying by one hot encoded target ensures that other (index != target) logit values become 0\n",
    "        # sum(1) simply gives us the logit value of the target class\n",
    "        real = (target * output).sum(1)\n",
    "        \n",
    "        # indices other than target class will have their logit values, target index will have -10000\n",
    "        # takes the maximum value when we suppress the logit of the targeted class\n",
    "        # this will give the logit of the most likely other class\n",
    "        # in the first run, this would most likely be the prob of the true class\n",
    "        other = ((1. - target) * output - target * 10000.).max(1)[0]\n",
    "        \n",
    "#         print('output: ', output)\n",
    "#         print('real: ', real)\n",
    "#         print('other: ', other)\n",
    "        \n",
    "#         print('dist shape: ', dist.shape)\n",
    "#         print('dist: ', dist)\n",
    "        \n",
    "        if self.targeted:\n",
    "            # if targeted, optimize for making the other class most likely\n",
    "            loss1 = torch.clamp(other - real + self.confidence, min=0.)  # equiv to max(..., 0.)\n",
    "        else:\n",
    "            # if non-targeted, optimize for making this class least likely.\n",
    "            loss1 = torch.clamp(real - other + self.confidence, min=0.)  # equiv to max(..., 0.)\n",
    "        \n",
    "        loss1 = torch.sum(scale_const * loss1)\n",
    "        loss2 = dist.sum()\n",
    "        \n",
    "        #print('loss2 which is dist.sum: ', loss2)\n",
    "\n",
    "        loss = loss1 + loss2\n",
    "        #print('loss: ', loss)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def _optimize(self, optimizer, model, input_var, modifier_var, target_var, scale_const_var, input_orig=None):\n",
    "        # apply modifier and clamp resulting image to keep bounded from clip_min to clip_max\n",
    "        if self.clamp_fn == 'tanh':\n",
    "            input_adv = tanh_rescale(modifier_var + input_var, self.clip_min, self.clip_max)\n",
    "        else:\n",
    "            input_adv = torch.clamp(modifier_var + input_var, self.clip_min, self.clip_max)\n",
    "        \n",
    "        #print('input_adv type:{} shape:{} min:{} max:{}'.format(type(input_adv),input_adv.shape, torch.min(input_adv),torch.max(input_adv)))\n",
    "        \n",
    "        output = model(input_adv)\n",
    "        index_base = np.argmax(output.cpu().data.numpy())\n",
    "        #print('Index_base: ',index_base)\n",
    "        #index_base_prob = torch.nn.functional.softmax(output)[0][index_base]\n",
    "        #print('Classification F(.) of the input is class: {} with probability:{}'.format(index_base, index_base_prob))\n",
    "\n",
    "        # distance to the original input data\n",
    "        if input_orig is None:\n",
    "            dist = l2_dist(input_adv, input_var, keepdim=False)\n",
    "        else:\n",
    "            dist = l2_dist(input_adv, input_orig, keepdim=False)\n",
    "            \n",
    "            \n",
    "\n",
    "        loss = self._loss(output, target_var, dist, scale_const_var)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #loss_np = loss.data[0] #throws error\n",
    "        loss_np = loss.data\n",
    "        dist_np = dist.data.cpu().numpy()\n",
    "        output_np = output.data.cpu().numpy()\n",
    "        input_adv_np = input_adv.data.permute(0, 2, 3, 1).cpu().numpy()  # back to BHWC for numpy consumption\n",
    "        return loss_np, dist_np, output_np, input_adv_np\n",
    "\n",
    "    def run(self, model, input, target, batch_idx=0):\n",
    "        batch_size = input.size(0)\n",
    "        #print('batch size: ', batch_size)\n",
    "        # set the lower and upper bounds accordingly\n",
    "        lower_bound = np.zeros(batch_size)\n",
    "        scale_const = np.ones(batch_size) * self.initial_const\n",
    "        upper_bound = np.ones(batch_size) * 1e10\n",
    "\n",
    "        # python/numpy placeholders for the overall best l2, label score, and adversarial image\n",
    "        o_best_l2 = [1e10] * batch_size\n",
    "        o_best_score = [-1] * batch_size\n",
    "        o_best_attack = input.permute(0, 2, 3, 1).cpu().numpy()\n",
    "\n",
    "        # setup input (image) variable, clamp/scale as necessary\n",
    "        if self.clamp_fn == 'tanh':\n",
    "            # convert to tanh-space, input already int -1 to 1 range, does it make sense to do\n",
    "            # this as per the reference implementation or can we skip the arctanh?\n",
    "            input_var = autograd.Variable(torch_arctanh(input), requires_grad=False)\n",
    "            input_orig = tanh_rescale(input_var, self.clip_min, self.clip_max)\n",
    "        else:\n",
    "            input_var = autograd.Variable(input, requires_grad=False)\n",
    "            input_orig = None\n",
    "\n",
    "        # setup the target variable, we need it to be in one-hot form for the loss function\n",
    "        target_onehot = torch.zeros(target.size() + (self.num_classes,))\n",
    "        if self.cuda:\n",
    "            target_onehot = target_onehot.cuda()\n",
    "        target_onehot.scatter_(1, target.unsqueeze(1), 1.)\n",
    "        \n",
    "        #target_onehot will have a 1 at the index of the targeted class (in the case of targeted attack)\n",
    "        #print('target_onehot: ', target_onehot)\n",
    "        \n",
    "        target_var = autograd.Variable(target_onehot, requires_grad=False)\n",
    "\n",
    "        # setup the modifier variable, this is the variable we are optimizing over\n",
    "        modifier = torch.zeros(input_var.size()).float()\n",
    "        if self.init_rand:\n",
    "            # Experiment with a non-zero starting point...\n",
    "            modifier = torch.normal(means=modifier, std=0.001)\n",
    "        if self.cuda:\n",
    "            modifier = modifier.cuda()\n",
    "        modifier_var = autograd.Variable(modifier, requires_grad=True)\n",
    "\n",
    "        optimizer = optim.Adam([modifier_var], lr=0.0005)\n",
    "\n",
    "        for search_step in range(self.binary_search_steps):\n",
    "            #print('Batch: {0:>3}, search step: {1}'.format(batch_idx, search_step))\n",
    "            if self.debug:\n",
    "                #print('Const:')\n",
    "                for i, x in enumerate(scale_const):\n",
    "                    print(i, x)\n",
    "            best_l2 = [1e10] * batch_size\n",
    "            best_score = [-1] * batch_size\n",
    "\n",
    "            # The last iteration (if we run many steps) repeat the search once.\n",
    "            if self.repeat and search_step == self.binary_search_steps - 1:\n",
    "                scale_const = upper_bound\n",
    "\n",
    "            scale_const_tensor = torch.from_numpy(scale_const).float()\n",
    "            if self.cuda:\n",
    "                scale_const_tensor = scale_const_tensor.cuda()\n",
    "            scale_const_var = autograd.Variable(scale_const_tensor, requires_grad=False)\n",
    "\n",
    "            prev_loss = 1e6\n",
    "            for step in range(self.max_steps):\n",
    "                # perform the attack\n",
    "                loss, dist, output, adv_img = self._optimize(\n",
    "                    optimizer,\n",
    "                    model,\n",
    "                    input_var,\n",
    "                    modifier_var,\n",
    "                    target_var,\n",
    "                    scale_const_var,\n",
    "                    input_orig)\n",
    "\n",
    "                #if step % 100 == 0 or step == self.max_steps - 1:\n",
    "                    #print('Step: {0:>4}, loss: {1:6.4f}, dist: {2:8.5f}, modifier mean: {3:.5e}'.format(step, loss, dist.mean(), modifier_var.data.mean()))\n",
    "\n",
    "                if self.abort_early and step % (self.max_steps // 10) == 0:\n",
    "                    if loss > prev_loss * .9999:\n",
    "                        print('Aborting early...')\n",
    "                        break\n",
    "                    prev_loss = loss\n",
    "\n",
    "                # update best result found\n",
    "                for i in range(batch_size):\n",
    "                    target_label = target[i]\n",
    "                    output_logits = output[i]\n",
    "                    output_label = np.argmax(output_logits)\n",
    "                    di = dist[i]\n",
    "                    if self.debug:\n",
    "                        if step % 100 == 0:\n",
    "                            print('{0:>2} dist: {1:.5f}, output: {2:>3}, {3:5.3}, target {4:>3}'.format(\n",
    "                                i, di, output_label, output_logits[output_label], target_label))\n",
    "                    if di < best_l2[i] and self._compare(output_logits, target_label):\n",
    "                        if self.debug:\n",
    "                            print('{0:>2} best step,  prev dist: {1:.5f}, new dist: {2:.5f}'.format(\n",
    "                                  i, best_l2[i], di))\n",
    "                        best_l2[i] = di\n",
    "                        best_score[i] = output_label\n",
    "                    if di < o_best_l2[i] and self._compare(output_logits, target_label):\n",
    "                        if self.debug:\n",
    "                            print('{0:>2} best total, prev dist: {1:.5f}, new dist: {2:.5f}'.format(\n",
    "                                  i, o_best_l2[i], di))\n",
    "                        o_best_l2[i] = di\n",
    "                        o_best_score[i] = output_label\n",
    "                        o_best_attack[i] = adv_img[i]\n",
    "\n",
    "                sys.stdout.flush()\n",
    "                # end inner step loop\n",
    "\n",
    "            # adjust the constants\n",
    "            batch_failure = 0\n",
    "            batch_success = 0\n",
    "            for i in range(batch_size):\n",
    "                if self._compare(best_score[i], target[i]) and best_score[i] != -1:\n",
    "                    # successful, do binary search and divide const by two\n",
    "                    upper_bound[i] = min(upper_bound[i], scale_const[i])\n",
    "                    if upper_bound[i] < 1e9:\n",
    "                        scale_const[i] = (lower_bound[i] + upper_bound[i]) / 2\n",
    "                    if self.debug:\n",
    "                        print('{0:>2} successful attack, lowering const to {1:.3f}'.format(\n",
    "                            i, scale_const[i]))\n",
    "                else:\n",
    "                    # failure, multiply by 10 if no solution found\n",
    "                    # or do binary search with the known upper bound\n",
    "                    lower_bound[i] = max(lower_bound[i], scale_const[i])\n",
    "                    if upper_bound[i] < 1e9:\n",
    "                        scale_const[i] = (lower_bound[i] + upper_bound[i]) / 2\n",
    "                    else:\n",
    "                        scale_const[i] *= 10\n",
    "                    if self.debug:\n",
    "                        print('{0:>2} failed attack, raising const to {1:.3f}'.format(\n",
    "                            i, scale_const[i]))\n",
    "                if self._compare(o_best_score[i], target[i]) and o_best_score[i] != -1:\n",
    "                    batch_success += 1\n",
    "                else:\n",
    "                    batch_failure += 1\n",
    "\n",
    "            print('Num failures: {0:2d}, num successes: {1:2d}\\n'.format(batch_failure, batch_success))\n",
    "            sys.stdout.flush()\n",
    "            # end outer search loop\n",
    "\n",
    "        return o_best_attack, o_best_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TARGETED = True\n",
    "MAX_STEPS = 1000\n",
    "SEARCH_STEPS = 6\n",
    "NO_CUDA = False\n",
    "DEBUG = False\n",
    "\n",
    "attack = AttackCarliniWagnerL2(\n",
    "        targeted=TARGETED,\n",
    "        max_steps=MAX_STEPS,\n",
    "        search_steps=SEARCH_STEPS,\n",
    "        cuda=not NO_CUDA,\n",
    "        debug=DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for batch_idx, (input, target) in enumerate(loader):\n",
    "    \n",
    "#     input = input.cuda()\n",
    "#     target = target.cuda()\n",
    "\n",
    "#     input_adv = attack.run(model, input, target, batch_idx)\n",
    "\n",
    "#     start_index = args.batch_size * batch_idx\n",
    "#     indices = list(range(start_index, start_index + input.size(0)))\n",
    "#     for filename, o in zip(dataset.filenames(indices, basename=True), input_adv):\n",
    "#         output_file = os.path.join(args.output_dir, filename)\n",
    "#         imsave(output_file, (o + 1.0) * 0.5, format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a,b = iter(test_loader).next()\n",
    "# a=a.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loader = test_loader\n",
    "\n",
    "iterations = 0\n",
    "thresh_on_iterations = 20\n",
    "\n",
    "source_class = []\n",
    "success_record = []\n",
    "best_l2s = []\n",
    "\n",
    "for batch_idx, (input_tensor, input_label) in enumerate(loader):\n",
    "    print('\\n\\n-------iteration: {}----------'.format(iterations))\n",
    "    source_class.extend(input_label.cpu().numpy())\n",
    "    # clean image\n",
    "    input_tensor = input_tensor.cuda() # size: ([batch_size, 1, 28, 28])\n",
    "    \n",
    "    # original label for the clean image\n",
    "    input_label = input_label.cuda() # size: ([batch_size])\n",
    "    \n",
    "    pred_input = model(input_tensor) # size: ([batch_size, num_classes])\n",
    "    pred_prob_input = F.softmax(pred_input, dim=1) # size: ([batch_size, num_classes])\n",
    "    pred_class = torch.argmax(pred_prob_input, dim=1) # size: ([batch_size])\n",
    "    #print('prediction of clean sample: {} with probability: {}'.format(torch.argmax(pred_prob_input, dim=1),torch.max(pred_prob_input, dim=1)))\n",
    "    \n",
    "    # if the attack is targeted, we will target the next class modulo number of classes\n",
    "    if TARGETED == True:\n",
    "        target = (input_label+1)%10\n",
    "        #print('target: ',target)\n",
    "    # else the target is kept as the original label as per attack design\n",
    "    else:\n",
    "        target = input_label\n",
    "    \n",
    "    # result obtained is a numpy array\n",
    "    adversarial_img, best_l2 = attack.run(model, input_tensor, target, batch_idx)\n",
    "    \n",
    "    best_l2s.extend(best_l2)\n",
    "    \n",
    "    # reshape\n",
    "    adversarial_img = np.transpose(adversarial_img, (0,3,1,2)) # size: ([batch_size, 1, 28, 28])\n",
    "    \n",
    "    # conver to torch tensor\n",
    "    adversarial_tensor = torch.from_numpy(adversarial_img).cuda() # size: ([batch_size, 1, 28, 28])\n",
    "    \n",
    "    # obtain the prediction by the model\n",
    "    pred_adv = model(adversarial_tensor) # size: ([batch_size, num_classes])\n",
    "    pred_prob_adv = F.softmax(pred_adv, dim=1) # size: ([batch_size, num_classes])\n",
    "    pred_class_adv = torch.argmax(pred_prob_adv, dim=1) # size: ([batch_size])\n",
    "#     print('prediction of adversarial sample: {} with probability: {}'.\n",
    "#       format(torch.argmax(pred_prob_adv),torch.max(pred_prob_adv)))\n",
    "\n",
    "    \n",
    "    #clean_label = input_label.item()\n",
    "    #pred_adv_label = torch.max(pred_adv, 1)[1].item()\n",
    "    #pred_adv_label = torch.argmax(pred_prob_adv)\n",
    "    \n",
    "#     print('Original label of the clean image is: {} and predicted label of the adversarial image is {}'\\\n",
    "#           .format(clean_label,pred_adv_label))\n",
    "    \n",
    "    \n",
    "    if TARGETED:\n",
    "        result = pred_class_adv == target\n",
    "    else:\n",
    "        result = pred_class_adv != input_label\n",
    "    \n",
    "    success_record.extend(result.cpu().numpy())\n",
    "    \n",
    "    iterations += 1\n",
    "    if iterations == thresh_on_iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('results/non_adaptive/success_record.npy',success_record)\n",
    "np.save('results/non_adaptive/best_l2s.npy',best_l2s)\n",
    "np.save('results/non_adaptive/source_class.npy',source_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000,)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "success_record = np.load('results/non_adaptive/success_record.npy')\n",
    "best_l2s = np.load('results/non_adaptive/best_l2s.npy')\n",
    "source_class = np.load('results/non_adaptive/source_class.npy')\n",
    "\n",
    "print(success_record.shape)\n",
    "print(best_l2s.shape)\n",
    "print(source_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.9525204"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(best_l2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_clean = input_tensor.cpu().numpy()\n",
    "plt.imshow(img_clean[0,0],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fed200674a8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAASh0lEQVR4nO3df4xV5ZkH8O8Dw8AwoIKDOvxQWEQjWbNUR1yj2WiabaxBoZpuirGyie70j5q0piZr3D/KX8botk3/WJtMVwLdoLVJayTG1BJCJKghjgSRKdJBw7ZTkOHX8EsUh3n2jzluRpzzPJf73nPPsc/3k5CZuc+857z33Hk499znvO8rqgoi+ts3oewOEFFzMNmJgmCyEwXBZCcKgslOFERLM3c2YcIEbWlp6i6JQhkeHsbIyIiMF0vKPBG5E8DPAUwE8N+q+pT1+y0tLZg1a1bKLukCiYz7un8leGVh77lVuayc8rpYz+vQoUO5sbrfxovIRAD/BeCbABYDWCkii+vdHhEVK+WafSmAvar6oaqeBfBrAMsb0y0iarSUZJ8D4C9jfh7IHvsCEekWkV4R6R0ZGUnYHRGlSEn28S46vnQxoao9qtqlql0TJvDDf6KypGTfAIB5Y36eC2B/WneIqCgpyf42gEUiskBEWgF8B8CGxnSLiBqt7tKbqg6LyCMAXsNo6W2NqvZ57aySg1cqseJFl2ms9t7lScrz8vbtKfrSqcjXLFWRZcevYlkwqc6uqq8CeLVBfSGiAvETM6IgmOxEQTDZiYJgshMFwWQnCoLJThRE0weXF1Vf9OqaRdY9U59TavtJkyblxqZOnWq2Ta3Dnz171ox//PHHubFz584l7Tul76nPu8yhw/Xum2d2oiCY7ERBMNmJgmCyEwXBZCcKgslOFESl5nX2SgpWuSS1fPXZZ5+Z8SKHaqaWDSdOnJgbGx4eNtu2traaca+05m2/qFlUa4mnDEu2jmktca9v1nHzpm+r9++NZ3aiIJjsREEw2YmCYLITBcFkJwqCyU4UBJOdKIim19lTppIuahpqwK+7WvtOXdYqdSrqlG17fU99bkVO/+1J6XtqrbuKq+fyzE4UBJOdKAgmO1EQTHaiIJjsREEw2YmCYLITBVGp8ewpY4xbWuynMmXKFDPutU+p2VrTKQP+mPGixjfXwhvnf/r06brbe8/bmiIb8KfJtsbqe9v2/l68abA//fRTM24p6vVMSnYR2QfgJIBzAIZVtasRnSKixmvEmf0OVT3cgO0QUYF4zU4URGqyK4A/iMg7ItI93i+ISLeI9IpIb+p91kRUv9S38beq6n4RuQzARhF5X1W3jP0FVe0B0AMAra2txX2SRESmpDO7qu7Pvg4CeAnA0kZ0iogar+5kF5F2EZn++fcAvgFgV6M6RkSNlfI2/nIAL2XjdlsAPK+qv/caWTVEbwywdc3vzePtzW9e5LjulDnEa2lv1Yy9fqcuZe3dG2Hdv+DVqr1at1crt/ruHRfvNUlV5DoEeepOdlX9EMA/NLAvRFQglt6IgmCyEwXBZCcKgslOFASTnSiISg1xTVma2CvTTJs2zYx7Q1wtXhnnyJEjZtwrIU2ePPmC+/Q573mdOXOm0Lg1vDd1umXvuFiveVtbm9nWKyl6w5a9UrBVdvRKkvVOqc4zO1EQTHaiIJjsREEw2YmCYLITBcFkJwqCyU4URKXq7F590ap9ekMSvSmRvbqoVdv09u3Vk724t32rTp+6lHXK8FrAfs1uuukms21fX58Z9+4hSFni2/tb9F6TlOnB29vbzbbWvQ3Wc+aZnSgIJjtREEx2oiCY7ERBMNmJgmCyEwXBZCcKolJ1dq9uatV0vdpkR0dH3dsG7LHTXk3V4z1vb/tW+5RjCgCrV6824xdddJEZ37p1a27slltuMds+88wzZtwbc26NWfeet8dbkvnUqVNm/OGHH86NLVu2zGx7//3358ZYZyciJjtRFEx2oiCY7ERBMNmJgmCyEwXBZCcKolJ1dm+MsTUm3Zu//Pjx42bcq0e3trbmxrya6yeffGLGPSnLKl955ZVm2xdffNGMP//882a8v7/fjE+dOjU35s0x8OSTT5rxxx57zIxbawl449G918yL33fffWZ8+fLluTFvjYNFixblxg4fPpwbc8/sIrJGRAZFZNeYx2aKyEYR6c++zvC2Q0TlquVt/FoAd5732OMANqnqIgCbsp+JqMLcZFfVLQCOnvfwcgDrsu/XAVjR4H4RUYPVe81+uaoeAABVPSAil+X9ooh0A+gG/HneiKg4hX8ar6o9qtqlql3ewAUiKk692XdQRDoBIPs62LguEVER6k32DQBWZd+vAvByY7pDREVxr9lF5AUAtwPoEJEBAD8G8BSA34jIQwD+DODbjeiM9zbfqoV76217dXQvbs3z7c0x7q3l7bn44ovN+Ouvv54bW7t2rdn22WefNePvvvuuGZ85c6YZtz6n2bNnj9n2qquuMuPbt2834+vXr8+NbdmyxWw7NDRkxq+55hozbo05B+x7DLz58j/66KO6tusmu6quzAl93WtLRNXBT8yIgmCyEwXBZCcKgslOFASTnSiISg1x9YY8WkM9vSGu3vBZq7QG2CWkxYsXm21nz55txh999FEzPjAwUHd7b6pnr28333yzGfeGelpTNnvH/K233jLjnnvvvTc3ds8995htjx07Zsa9Ycf79+834zt27MiN3XjjjWbb6dOn58asv1Oe2YmCYLITBcFkJwqCyU4UBJOdKAgmO1EQTHaiICpVZ/dql1Zd1qvZevXghQsXmnFrmOndd99ttvWGaj799NNm3KqrAsCcOXNyYzNm2BP/esfcWy7ai1vb96Yp84Ydv/baa2b8jTfeyI15r8ncuXPN+IkTJ8z4m2++acatJZ1XrLCndLTuR7HuJ+GZnSgIJjtREEx2oiCY7ERBMNmJgmCyEwXBZCcKoql19tbWVixYsCA37tUXjx49f8m5L27bYi0dDNjjrgF7fPPmzZvNtl692BsPn8JbTto7bpdccokZ95aEto67t+/Uvln3VnzwwQdm261bt5rxwUF7XRTv/obbbrstN+bV8K+99trc2N69e3NjPLMTBcFkJwqCyU4UBJOdKAgmO1EQTHaiIJjsREE0tc7e0dGBVatW5cavu+46s701dtpb7tlbVvnw4cNm3Jp/fcqUKWZba4ldADh58qQZ98ZeX3311bmxefPmmW29WrV3/4E3H7/1ugwPD5ttvXUEvPY7d+7Mjd1www1m2wceeMCMe8twv//++2bcGufvHdN9+/blxqz7Ktwzu4isEZFBEdk15rHVIvJXEdmR/bvL2w4RlauWt/FrAdw5zuM/U9Ul2b9XG9stImo0N9lVdQuA/PtUiegrIeUDukdEZGf2Nj/3RmAR6RaRXhHp9a5Niag49Sb7LwAsBLAEwAEAP8n7RVXtUdUuVe3yJk4kouLUleyqelBVz6nqCIBfAlja2G4RUaPVlewi0jnmx28B2JX3u0RUDW6dXUReAHA7gA4RGQDwYwC3i8gSAApgH4Dv1bKzoaEhvPLKK7lxa7w6AFxxxRW5Me8Sob293YzPmjXLjLe1teXGrDH6gF9H9+Yo957b5MmTc2NezdYbO+3p7e0149b86d7a8cuWLTPjDz74oBm3nrs3Vt6bg8Cb896bj98aa+/dE2L9PVj3HrjJrqorx3n4Oa8dEVULb5clCoLJThQEk50oCCY7URBMdqIgxCvNNFJbW5vOnz8/N+4NFbWm7/WWbPaGwHr7toaZzp4922x7xx13mPGNGzea8T179phxq4xz5swZs633+nslJG+qamv73mvmlTSvv/56M97X15cb815va0llwC9Zes/NKpd6pdZp06blxrZt24YTJ06M+6LxzE4UBJOdKAgmO1EQTHaiIJjsREEw2YmCYLITBdHUqaTPnTtn1ie9YajWMrfekEOvnnz8+HEz3t/fnxvr6Ogw265du9aMe8sHe8MxrSGyXs3WOy7eUE9vOKa1fW8q6CNHjphxr8Zv3d/gHfNLL73UjJ8+fdqMe8/NmqLby4OhoSEznodndqIgmOxEQTDZiYJgshMFwWQnCoLJThQEk50oiKaOZ29tbVVvymaLN0bYsnDhQjPujW+26qKdnZ25McCvF3u1cK/ObtW6vWPmLYvsjds+duyYGffmEbB49Wavlm1NPZ46zt+7/8A77tZxteYnAOwlmwcHB3H27FmOZyeKjMlOFASTnSgIJjtREEx2oiCY7ERBMNmJgqhUnT2lL9785l6916ubenGLVw/2xmV7+7aeu3dcvGPu1eG9uFVv9vrmzVHg3Rthxb17F7zXzIt7dXarvfeaWNtOqrOLyDwR2Swiu0WkT0R+kD0+U0Q2ikh/9nWGty0iKk8tb+OHAfxIVa8D8I8Avi8iiwE8DmCTqi4CsCn7mYgqyk12VT2gqtuz708C2A1gDoDlANZlv7YOwIqiOklE6S7oQlRE5gP4GoBtAC5X1QPA6H8IInJZTptuAN2Afw1GRMWp+dN4EZkG4LcAfqiq9uiIMVS1R1W7VLUrZVAEEaWpKftEZBJGE329qv4ue/igiHRm8U4A+UusElHp3LfxMlofeQ7AblX96ZjQBgCrADyVfX05tTNeKSalbeq7Cqsc4k2nnDLdMuCXeYp8x+Rt2zvu1tDg1GW2Pdb2U4ZLp+7bU1Q5vJZr9lsBfBfAeyKyI3vsCYwm+W9E5CEAfwbw7UJ6SEQN4Sa7qm4FkPff99cb2x0iKgo/MSMKgslOFASTnSgIJjtREEx2oiCaumRzkVKHcqbUNlOXi05tn8Lbdur9C9b2vW17teoih5Gm9s1T5LDkPDyzEwXBZCcKgslOFASTnSgIJjtREEx2oiCY7ERB/M3U2VNr0SlLG6fW8FNrttb2U4+LV0dPmWqszNfMm2OgqFp3LYoaa88zO1EQTHaiIJjsREEw2YmCYLITBcFkJwqCyU4URKXq7CnzxpfJq4umPq+it5+yb0+RfUsZk546nr3I+RM4np2IkjDZiYJgshMFwWQnCoLJThQEk50oCCY7URC1rM8+D8CvAFwBYARAj6r+XERWA/g3AIeyX31CVV8tqqOpUsecp8zz7UldAz1l/6nHJbVeXaSUcf5FjldP3X69x7SWm2qGAfxIVbeLyHQA74jIxiz2M1X9z7r2TERNVcv67AcAHMi+PykiuwHMKbpjRNRYF3TNLiLzAXwNwLbsoUdEZKeIrBGRGTltukWkV0R6i5puh4h8NSe7iEwD8FsAP1TVEwB+AWAhgCUYPfP/ZLx2qtqjql2q2uVdmxJRcWrKPhGZhNFEX6+qvwMAVT2oqudUdQTALwEsLa6bRJTKTXYZ/ejvOQC7VfWnYx7vHPNr3wKwq/HdI6JGqeXT+FsBfBfAeyKyI3vsCQArRWQJAAWwD8D3UjtTRjmiClL7XmSZqMiyX5nK7ncZ+6/l0/itAMbrWWVr6kT0ZfzEjCgIJjtREEx2oiCY7ERBMNmJgmCyEwVRqamkU6QOtSx7yKMlpdZd9BDUIu+NKPOYp6ri0F+e2YmCYLITBcFkJwqCyU4UBJOdKAgmO1EQTHaiIKSZtUwROQTgf8c81AHgcNM6cGGq2req9gtg3+rVyL5dpaqzxgs0Ndm/tHORXlXtKq0Dhqr2rar9Ati3ejWrb3wbTxQEk50oiLKTvafk/Vuq2req9gtg3+rVlL6Ves1ORM1T9pmdiJqEyU4URCnJLiJ3isgeEdkrIo+X0Yc8IrJPRN4TkR0i0ltyX9aIyKCI7Brz2EwR2Sgi/dnXcdfYK6lvq0Xkr9mx2yEid5XUt3kisllEdotIn4j8IHu81GNn9Kspx63p1+wiMhHAnwD8M4ABAG8DWKmqf2xqR3KIyD4AXapa+g0YIvJPAE4B+JWq/n322NMAjqrqU9l/lDNU9d8r0rfVAE6VvYx3tlpR59hlxgGsAPCvKPHYGf36FzThuJVxZl8KYK+qfqiqZwH8GsDyEvpReaq6BcDR8x5eDmBd9v06jP6xNF1O3ypBVQ+o6vbs+5MAPl9mvNRjZ/SrKcpI9jkA/jLm5wFUa713BfAHEXlHRLrL7sw4LlfVA8DoHw+Ay0ruz/ncZbyb6bxlxitz7OpZ/jxVGck+3uRbVar/3aqqNwD4JoDvZ29XqTY1LePdLOMsM14J9S5/nqqMZB8AMG/Mz3MB7C+hH+NS1f3Z10EAL6F6S1Ef/HwF3ezrYMn9+X9VWsZ7vGXGUYFjV+by52Uk+9sAFonIAhFpBfAdABtK6MeXiEh79sEJRKQdwDdQvaWoNwBYlX2/CsDLJfblC6qyjHfeMuMo+diVvvy5qjb9H4C7MPqJ/AcA/qOMPuT06+8AvJv96yu7bwBewOjbus8w+o7oIQCXAtgEoD/7OrNCffsfAO8B2InRxOosqW+3YfTScCeAHdm/u8o+dka/mnLceLssURC8g44oCCY7URBMdqIgmOxEQTDZiYJgshMFwWQnCuL/ALMvIJ9P48XnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(adversarial_img[0,0],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_input = model(input_tensor)\n",
    "pred_prob_input = F.softmax(pred_input, dim=1)\n",
    "print('prediction of adversarial sample: {} with probability: {}'\n",
    "      .format(torch.argmax(pred_prob_input),torch.max(pred_prob_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_prob_adv = F.softmax(pred_adv, dim=1)\n",
    "print('prediction of adversarial sample: {} with probability: {}'.\n",
    "      format(torch.argmax(pred_prob_adv),torch.max(pred_prob_adv)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_onehot_ar = np.array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
    "target_onehot = torch.from_numpy(target_onehot_ar)\n",
    "target_onehot = target_onehot.cuda()\n",
    "\n",
    "output_ar = np.array([[-5.3372, -5.6212, -4.7658, -3.9166, -2.1613, -3.9361, -6.2410, -2.7965,-2.5990, -0.3694]])\n",
    "output = torch.from_numpy(output_ar)\n",
    "output = output.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(target_onehot * output).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real = (target_onehot * output).sum(1)\n",
    "real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other = ((1. - target_onehot) * output - target_onehot * 10000.)\n",
    "other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other = (other).max(1)[0]\n",
    "other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other - real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.clamp(other - real, min=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# target_onehot:  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
    "# Batch:   0, search step: 0\n",
    "# output:  tensor([[-5.3372, -5.6212, -4.7658, -3.9166, -2.1613, -3.9361, -6.2410, -2.7965,\n",
    "#          -2.5990, -0.3694]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
    "real:  tensor([-5.3372], device='cuda:0', grad_fn=<SumBackward1>)\n",
    "other:  tensor([-0.3694], device='cuda:0', grad_fn=<MaxBackward0>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real = (target * output).sum(1)\n",
    "other = ((1. - target) * output - target * 10000.).max(1)[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
