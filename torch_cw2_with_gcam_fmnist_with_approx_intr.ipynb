{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torch.utils import data as D\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "from skimage.transform import resize\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 1\n",
    "PATCH_SIZE = 14 # it is 64 for 224x224 image\n",
    "IMG_W = 28 # 224 originally\n",
    "IMG_H = 28 # 224 originally\n",
    "# MEANS = np.array([0.1307])\n",
    "# STDS = np.array([0.3081])\n",
    "MEANS = np.array([0.])\n",
    "STDS = np.array([1.])\n",
    "\n",
    "dataset = 'fmnist'\n",
    "source_class = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilons = [0, .05, .1, .15, .2, .25, .3]\n",
    "use_cuda=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SaliencyModel(\n",
       "  (encoder): ResNetEncoder(\n",
       "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  )\n",
       "  (up2): UNetUpsampler(\n",
       "    (upsampler): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): PixelShuffleBlock()\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (follow_up): Bottleneck(\n",
       "      (conv1): Conv2d(1280, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation_fn): ReLU()\n",
       "      (residual_transformer): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (up1): UNetUpsampler(\n",
       "    (upsampler): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): PixelShuffleBlock()\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (follow_up): Bottleneck(\n",
       "      (conv1): Conv2d(640, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation_fn): ReLU()\n",
       "      (residual_transformer): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (up0): UNetUpsampler(\n",
       "    (upsampler): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): PixelShuffleBlock()\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (follow_up): Bottleneck(\n",
       "      (conv1): Conv2d(320, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation_fn): ReLU()\n",
       "      (residual_transformer): Conv2d(320, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (to_saliency_chans): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from  torch.optim import lr_scheduler\n",
    "from sal.nn_network import SaliencyModel\n",
    "from sal.utils.resnet_encoder import resnet50encoder\n",
    "from  torch.optim import lr_scheduler\n",
    "\n",
    "num_epochs = 1\n",
    "lr         = 1e-3\n",
    "momentum   = 0.9\n",
    "w_decay    = 1e-5\n",
    "step_size  = 5\n",
    "gamma      = 0.5\n",
    "\n",
    "channel_dims=[64,256,512,1024,2048]\n",
    "intr_model= SaliencyModel(resnet50encoder(pretrained=False), channel_dims)\n",
    "\n",
    "criterion = nn.MSELoss().cuda()\n",
    "optimizer = optim.Adam(intr_model.parameters(), lr=lr, weight_decay=w_decay)\n",
    "scheduler_nn=lr_scheduler.StepLR(optimizer,step_size=step_size,gamma=gamma)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pretrained_intr_model = './model/newnet_new.pth'\n",
    "\n",
    "intr_model.eval()\n",
    "intr_model = torch.load(pretrained_intr_model, map_location=device)\n",
    "intr_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, features, num_classes, init_weights=True):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4*4*50, 500),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(500, num_classes)\n",
    "        )\n",
    "        \n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        # x are the logits values\n",
    "        return x \n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, \n",
    "stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "\"\"\"\n",
    "\n",
    "def make_layers(cfg, in_channels, kernel_size, stride, padding, batch_norm=False):\n",
    "    layers = []\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=kernel_size, padding=padding)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Refer VGG19_bn configurationh here: \n",
    "https://github.com/pytorch/vision/blob/76702a03d6cc2e4f431bfd1914d5e301c07bd489/torchvision/models/vgg.py#L63\n",
    "\"\"\"\n",
    "cfgs = {\n",
    "    #'E': [64, 64, 'M',128, 128, 'M',256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M',512, 512, 512, 512, 'M'],\n",
    "    'E': [20, 'M', 50, 'M']\n",
    "}\n",
    "\n",
    "model_layers = make_layers(cfgs['E'],in_channels=1, kernel_size=5, stride=1, padding=0, batch_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \"\"\" Class for extracting activations and\n",
    "    registering gradients from targeted intermediate layers \"\"\"\n",
    "    def __init__(self, model, target_layers):\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.gradients = []\n",
    "\n",
    "    def save_gradient(self, grad):\n",
    "        self.gradients.append(grad)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outputs = []\n",
    "        self.gradients = []\n",
    "        for name, module in self.model._modules.items():\n",
    "            x = module(x)\n",
    "            if name in self.target_layers:\n",
    "                # Store the features and register hook to save gradients for the target layer\n",
    "                x.register_hook(self.save_gradient)\n",
    "                outputs += [x]\n",
    "        return outputs, x\n",
    "\n",
    "class ModelOutputs:\n",
    "    \"\"\" Class for making a forward pass, and getting:\n",
    "    1. The network output.\n",
    "    2. Activations from intermediate targeted layers.\n",
    "    3. Gradients from intermediate targeted layers. \"\"\"\n",
    "    def __init__(self, model, target_layers):\n",
    "        self.model = model\n",
    "        self.feature_extractor = FeatureExtractor(self.model.features, target_layers)\n",
    "\n",
    "    def get_gradients(self):\n",
    "        # Retrieve the saved gradients for the target layer\n",
    "        return self.feature_extractor.gradients\n",
    "\n",
    "    def __call__(self, x):\n",
    "        target_activations, output = self.feature_extractor(x)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.model.classifier(output)\n",
    "        return target_activations, output\n",
    "\n",
    "class GradCam:\n",
    "    \"\"\"\n",
    "    This class computes the Grad-CAM mask for the specified index.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_layer_names):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.model = model.cuda()\n",
    "        self.extractor = ModelOutputs(self.model, target_layer_names)\n",
    "\n",
    "    def __call__(self, image_tensor, index=None):\n",
    "        features, output = self.extractor(image_tensor)\n",
    "\n",
    "        if index is None:\n",
    "            index = np.argmax(output.cpu().data.numpy())\n",
    "\n",
    "        # Compute the one-hot tensor corresponding to the index\n",
    "        one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
    "        one_hot[0][index] = 1\n",
    "        one_hot = Variable(torch.from_numpy(one_hot), requires_grad=True)\n",
    "        one_hot = torch.sum(one_hot.cuda() * output)\n",
    "\n",
    "        self.model.features.zero_grad()\n",
    "        self.model.classifier.zero_grad()\n",
    "        one_hot.backward(retain_graph=True)\n",
    "\n",
    "        # Get the gradients and features to compute Grad-CAM\n",
    "        grads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()\n",
    "        target = features[-1]\n",
    "        target = target.cpu().data.numpy()[0, :]\n",
    "\n",
    "        weights = np.mean(grads_val, axis=(2, 3))[0, :]\n",
    "        cam = np.zeros(target.shape[1:], dtype=np.float32)\n",
    "\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * target[i, :, :]\n",
    "\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = cv2.resize(cam, (IMG_W, IMG_W))\n",
    "        cam = cam - np.min(cam)\n",
    "        cam = cam / np.max(cam)\n",
    "        return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomDS(D.Dataset):\n",
    "    \"\"\"\n",
    "    A customized data loader.\n",
    "    \"\"\"\n",
    "    def __init__(self, path, train=True):\n",
    "        \"\"\" Intialize the dataset\n",
    "        \"\"\"\n",
    "        if train:\n",
    "            data_path = os.path.join(path,'x_train.npy')\n",
    "            targets_path = os.path.join(path,'y_train.npy')\n",
    "        else:\n",
    "            data_path = os.path.join(path,'x_test.npy')\n",
    "            targets_path = os.path.join(path,'y_test.npy')\n",
    "\n",
    "        self.path = data_path\n",
    "        self.data = np.load(data_path)\n",
    "        self.targets = np.load(targets_path)\n",
    "        #self.transform = transforms.ToTensor()\n",
    "        self.transform = transforms.Compose([\n",
    "                       transforms.ToTensor()\n",
    "                       #transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])\n",
    "        self.len = np.shape(self.data)[0]\n",
    "        \n",
    "    # You must override __getitem__ and __len__\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset\n",
    "        \"\"\"\n",
    "        data = self.data[index]\n",
    "        #image = Image.fromarray(data)\n",
    "        \n",
    "        \n",
    "        target = int(self.targets[index])\n",
    "        \n",
    "        #data = (data * 255).astype(np.uint8)\n",
    "        #data = data.reshape(28,28)\n",
    "        #image = Image.fromarray((data * 255).astype(np.uint8))\n",
    "        #image = Image.fromarray(data.astype(np.uint8))\n",
    "        \n",
    "        return self.transform(data), target\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape : (60000, 28, 28)\n",
      "y_train shape : (60000,)\n",
      "x_test shape : (10000, 28, 28)\n",
      "y_test shape : (10000,)\n",
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "directory = './data/fmnist'\n",
    "\n",
    "IS_DATA_READY = True\n",
    "assert(IS_DATA_READY == True)\n",
    "\n",
    "x_train = np.load(directory + '/x_train.npy')\n",
    "y_train = np.load(directory + '/y_train.npy')\n",
    "x_test = np.load(directory + '/x_test.npy')\n",
    "y_test = np.load(directory + '/y_test.npy')\n",
    "print('x_train shape : {}'.format(x_train.shape))\n",
    "print('y_train shape : {}'.format(y_train.shape))\n",
    "print('x_test shape : {}'.format(x_test.shape))\n",
    "print('y_test shape : {}'.format(y_test.shape))\n",
    "\n",
    "\n",
    "# Simple dataset. Only save path to image and load it and transform to tensor when call __getitem__.\n",
    "filepath = './data/fmnist/'\n",
    "train_set = CustomDS(filepath, train=True)\n",
    "test_set = CustomDS(filepath, train=False)\n",
    "\n",
    "# total images in set\n",
    "print(train_set.len)\n",
    "print(test_set.len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# main method\n",
    "## Training settings\n",
    "# input batch size for training (default: 64)\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "# input batch size for testing (default: 1000)\n",
    "TEST_BATCH_SIZE = 1\n",
    "\n",
    "# number of epochs to train\n",
    "EPOCHS = 10\n",
    "\n",
    "#learning rate (default: 0.01)\n",
    "LR = 0.01\n",
    "\n",
    "#SGD momentum (default: 0.5)\n",
    "MOMENTUM = 0.5\n",
    "\n",
    "# how many batches to wait before logging training status\n",
    "LOG_INTERVAL = 10\n",
    "\n",
    "SAVE_MODEL = True\n",
    "SEED = 1\n",
    "NO_CUDA = False\n",
    "USE_CUDA = not NO_CUDA and torch.cuda.is_available()\n",
    "\n",
    "NUM_CLASSES=10\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if USE_CUDA else {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import operator as op\n",
    "import functools as ft\n",
    "\n",
    "\n",
    "'''reduce_* helper functions reduce tensors on all dimensions but the first.\n",
    "They are intended to be used on batched tensors where dim 0 is the batch dim.\n",
    "'''\n",
    "\n",
    "\n",
    "def reduce_sum(x, keepdim=True):\n",
    "    # silly PyTorch, when will you get proper reducing sums/means?\n",
    "    for a in reversed(range(1, x.dim())):\n",
    "        x = x.sum(a, keepdim=keepdim)\n",
    "    return x\n",
    "\n",
    "\n",
    "def reduce_mean(x, keepdim=True):\n",
    "    numel = ft.reduce(op.mul, x.size()[1:])\n",
    "    x = reduce_sum(x, keepdim=keepdim)\n",
    "    return x / numel\n",
    "\n",
    "\n",
    "def reduce_min(x, keepdim=True):\n",
    "    for a in reversed(range(1, x.dim())):\n",
    "        x = x.min(a, keepdim=keepdim)[0]\n",
    "    return x\n",
    "\n",
    "\n",
    "def reduce_max(x, keepdim=True):\n",
    "    for a in reversed(range(1, x.dim())):\n",
    "        x = x.max(a, keepdim=keepdim)[0]\n",
    "    return x\n",
    "\n",
    "\n",
    "def torch_arctanh(x, eps=1e-6):\n",
    "    x *= (1. - eps)\n",
    "    return (torch.log((1 + x) / (1 - x))) * 0.5\n",
    "\n",
    "\n",
    "def l2r_dist(x, y, keepdim=True, eps=1e-8):\n",
    "    d = (x - y)**2\n",
    "    d = reduce_sum(d, keepdim=keepdim)\n",
    "    d += eps  # to prevent infinite gradient at 0\n",
    "    return d.sqrt()\n",
    "\n",
    "\n",
    "def l2_dist(x, y, keepdim=True):\n",
    "    d = (x - y)**2\n",
    "    return reduce_sum(d, keepdim=keepdim)\n",
    "\n",
    "\n",
    "def l1_dist(x, y, keepdim=True):\n",
    "    d = torch.abs(x - y)\n",
    "    return reduce_sum(d, keepdim=keepdim)\n",
    "\n",
    "\n",
    "def l2_norm(x, keepdim=True):\n",
    "    norm = reduce_sum(x*x, keepdim=keepdim)\n",
    "    return norm.sqrt()\n",
    "\n",
    "\n",
    "def l1_norm(x, keepdim=True):\n",
    "    return reduce_sum(x.abs(), keepdim=keepdim)\n",
    "\n",
    "\n",
    "def rescale(x, x_min=-1., x_max=1.):\n",
    "    return x * (x_max - x_min) + x_min\n",
    "\n",
    "\n",
    "def tanh_rescale(x, x_min=-1., x_max=1.):\n",
    "    return (torch.tanh(x) + 1) * 0.5 * (x_max - x_min) + x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"PyTorch Carlini and Wagner L2 attack algorithm.\n",
    "\n",
    "Based on paper by Carlini & Wagner, https://arxiv.org/abs/1608.04644 and a reference implementation at\n",
    "https://github.com/tensorflow/cleverhans/blob/master/cleverhans/attacks_tf.py\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from torch import autograd\n",
    "#from .helpers import *\n",
    "\n",
    "\n",
    "class AttackCarliniWagnerL2:\n",
    "\n",
    "    def __init__(self, targeted=True, search_steps=None, max_steps=None, cuda=True, debug=False):\n",
    "        self.debug = debug\n",
    "        self.targeted = targeted\n",
    "        #self.num_classes = 1000\n",
    "        self.num_classes = 10\n",
    "        self.confidence = 0  # FIXME need to find a good value for this, 0 value used in paper not doing much...\n",
    "        self.initial_const = 0.1  # bumped up from default of .01 in reference code\n",
    "        self.binary_search_steps = search_steps or 5\n",
    "        self.repeat = self.binary_search_steps >= 10\n",
    "        self.max_steps = max_steps or 1000\n",
    "        self.abort_early = True\n",
    "        self.clip_min = -1.\n",
    "        self.clip_max = 1.\n",
    "        self.cuda = cuda\n",
    "        self.clamp_fn = 'tanh'  # set to something else perform a simple clamp instead of tanh\n",
    "        self.init_rand = False  # an experiment, does a random starting point help?\n",
    "\n",
    "    def _compare(self, output, target):\n",
    "        if not isinstance(output, (float, int, np.int64)):\n",
    "            output = np.copy(output)\n",
    "            if self.targeted:\n",
    "                output[target] -= self.confidence\n",
    "            else:\n",
    "                output[target] += self.confidence\n",
    "            output = np.argmax(output)\n",
    "        if self.targeted:\n",
    "            return output == target\n",
    "        else:\n",
    "            return output != target\n",
    "\n",
    "    def _loss(self, output, target, dist, scale_const, output_exp, target_exp):\n",
    "        # compute the probability of the label class versus the maximum other\n",
    "        \n",
    "        # for the targeted attack, real will contain the current logit values for the targeted class\n",
    "        # This basically tell us what is the current probability of the image being classified as the target class\n",
    "        # multiplying by one hot encoded target ensures that other (index != target) logit values become 0\n",
    "        # sum(1) simply gives us the logit value of the target class\n",
    "        real = (target * output).sum(1)\n",
    "        real_exp = (target_exp * output_exp).sum(1)\n",
    "        \n",
    "        # indices other than target class will have their logit values, target index will have -10000\n",
    "        # takes the maximum value when we suppress the logit of the targeted class\n",
    "        # this will give the logit of the most likely other class\n",
    "        # in the first run, this would most likely be the prob of the true class\n",
    "        other = ((1. - target) * output - target * 10000.).max(1)[0]\n",
    "        other_exp = ((1. - target_exp) * output_exp - target_exp * 10000.).max(1)[0]\n",
    "        \n",
    "#         print('output: ', output)\n",
    "#         print('real: ', real)\n",
    "#         print('other: ', other)\n",
    "        \n",
    "#         print('dist shape: ', dist.shape)\n",
    "#         print('dist: ', dist)\n",
    "        \n",
    "#         print('output exp: ', output_exp)\n",
    "#         print('real exp: ', real_exp)\n",
    "#         print('other exp: ', other_exp)\n",
    "        \n",
    "        \n",
    "        if self.targeted:\n",
    "            # if targeted, optimize for making the other class most likely\n",
    "            loss1 = torch.clamp(other - real + self.confidence, min=0.)  # equiv to max(..., 0.)\n",
    "        else:\n",
    "            # if non-targeted, optimize for making this class least likely.\n",
    "            loss1 = torch.clamp(real - other + self.confidence, min=0.)  # equiv to max(..., 0.)\n",
    "        \n",
    "        loss1 = torch.sum(scale_const * loss1)\n",
    "        loss2 = dist.sum()\n",
    "        \n",
    "        loss3 = torch.clamp(other_exp - real_exp + self.confidence, min=0.) \n",
    "        loss3 = torch.sum(scale_const * loss3)\n",
    "        \n",
    "        #print('loss2 which is dist.sum: ', loss2)\n",
    "\n",
    "        loss = loss1 + loss2 + loss3\n",
    "        #loss = loss3\n",
    "        #loss = loss1 + loss2\n",
    "        \n",
    "#         print('loss1: ',loss1)\n",
    "#         print('loss2: ',loss2)\n",
    "#         print('loss3: ', loss3)\n",
    "#         print('total loss is: ', loss)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def _optimize(self, optimizer, model, exp_model, intr_model, input_var,\n",
    "                  modifier_var, target, target_var, target_var_exp, \n",
    "                  scale_const_var, gradcam, input_orig=None):\n",
    "        # apply modifier and clamp resulting image to keep bounded from clip_min to clip_max\n",
    "        if self.clamp_fn == 'tanh':\n",
    "            input_adv = tanh_rescale(modifier_var + input_var, self.clip_min, self.clip_max)\n",
    "        else:\n",
    "            input_adv = torch.clamp(modifier_var + input_var, self.clip_min, self.clip_max)\n",
    "        \n",
    "        # input_adv is between [-1,1]\n",
    "        # output logits from the base model f(.)\n",
    "        output = model(input_adv)\n",
    "#         index_base = np.argmax(output.cpu().data.numpy())\n",
    "#         index_base_prob = torch.nn.functional.softmax(output)[0][index_base]\n",
    "#         print('Classification F(.) of the input is class: {} with probability:{}'.format(index_base, index_base_prob))\n",
    "\n",
    "        # distance to the original input data\n",
    "        if input_orig is None:\n",
    "            dist = l2_dist(input_adv, input_var, keepdim=False)\n",
    "        else:\n",
    "            dist = l2_dist(input_adv, input_orig, keepdim=False)\n",
    "            \n",
    "        # obtain the gradcam output for the adversarial image tensor\n",
    "        input_tensor = nn.Upsample(scale_factor=8, mode='bilinear')(input_adv)\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        output_intr = intr_model(input_tensor) # shape : torch.Size([1, 1, 224, 224])\n",
    "        ave_p = nn.AvgPool2d(kernel_size=8, stride=None, padding=0)\n",
    "        input_intr_tensor = ave_p(output_intr) #[1,1,28,28]\n",
    "\n",
    "        # obtain prediction\n",
    "        # Normalize -1 to 1....0 to 1 # TODO: see if this is not compatible with tanh thing done to input \n",
    "        input_intr_tensor = (input_intr_tensor - torch.min(input_intr_tensor))/(torch.max(input_intr_tensor) - torch.min(input_intr_tensor))\n",
    "        output_exp = exp_model(input_intr_tensor)\n",
    "#         index = np.argmax(output_exp.cpu().data.numpy())\n",
    "#         index_prob = torch.nn.functional.softmax(output_exp)[0][index]\n",
    "#         print('Classification G(.) of the explanation is class: {} with probability:{}'.format(index, index_prob))\n",
    "        \n",
    "        loss = self._loss(output, target_var, dist, scale_const_var, output_exp, target_var_exp)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #loss_np = loss.data[0] #throws error\n",
    "        loss_np = loss.data\n",
    "        dist_np = dist.data.cpu().numpy()\n",
    "        output_np = output.data.cpu().numpy()\n",
    "        output_exp_np = output_exp.data.cpu().numpy()\n",
    "        input_adv_np = input_adv.data.permute(0, 2, 3, 1).cpu().numpy()  # back to BHWC for numpy consumption\n",
    "        return loss_np, dist_np, output_np, output_exp_np, input_adv_np\n",
    "\n",
    "    def run(self, model, exp_model, intr_model, input, target, gradcam, batch_idx=0):\n",
    "        batch_size = input.size(0)\n",
    "        # print('batch size: ', batch_size)\n",
    "        # set the lower and upper bounds accordingly\n",
    "        lower_bound = np.zeros(batch_size)\n",
    "        scale_const = np.ones(batch_size) * self.initial_const\n",
    "        upper_bound = np.ones(batch_size) * 1e10\n",
    "\n",
    "        # python/numpy placeholders for the overall best l2, label score, and adversarial image\n",
    "        o_best_l2 = [1e10] * batch_size\n",
    "        o_best_score = [-1] * batch_size\n",
    "        o_best_attack = input.permute(0, 2, 3, 1).cpu().numpy()\n",
    "\n",
    "        # setup input (image) variable, clamp/scale as necessary\n",
    "        if self.clamp_fn == 'tanh':\n",
    "            # convert to tanh-space, input already int -1 to 1 range, does it make sense to do\n",
    "            # this as per the reference implementation or can we skip the arctanh?\n",
    "            input_var = autograd.Variable(torch_arctanh(input), requires_grad=False)\n",
    "            input_orig = tanh_rescale(input_var, self.clip_min, self.clip_max)\n",
    "        else:\n",
    "            input_var = autograd.Variable(input, requires_grad=False)\n",
    "            input_orig = None\n",
    "\n",
    "        # setup the target variable, we need it to be in one-hot form for the loss function\n",
    "        target_onehot = torch.zeros(target.size() + (self.num_classes,))\n",
    "        \n",
    "        if self.cuda:\n",
    "            target_onehot = target_onehot.cuda()\n",
    "        target_onehot.scatter_(1, target.unsqueeze(1), 1.)\n",
    "        \n",
    "        #target_onehot will have a 1 at the index of the targeted class (in the case of targeted attack)\n",
    "        #print('target_onehot: ', target_onehot)\n",
    "        \n",
    "        target_var = autograd.Variable(target_onehot, requires_grad=False)\n",
    "        \n",
    "        \n",
    "        ## setup the target variable for the explanation based model\n",
    "        # target for the explanation based model is the negative class 0 i.e., one-hot target is [1, 0]\n",
    "        target_exp = torch.tensor([0]).cuda()\n",
    "        target_onehot_exp = torch.zeros(target_exp.size() + (2,)) # 2 is the num of classes in exp model\n",
    "         \n",
    "        if self.cuda:\n",
    "            target_onehot_exp = target_onehot_exp.cuda()\n",
    "        target_onehot_exp.scatter_(1, target_exp.unsqueeze(1), 1.)\n",
    "        \n",
    "        #target_onehot will have a 1 at the index of the targeted class (in the case of targeted attack)\n",
    "#         print('target_onehot for base model: ', target_onehot)\n",
    "#         print('target_onehot for exp model: ', target_onehot_exp)\n",
    "        \n",
    "        target_var_exp = autograd.Variable(target_onehot_exp, requires_grad=False)\n",
    "\n",
    "\n",
    "        # setup the modifier variable, this is the variable we are optimizing over\n",
    "        modifier = torch.zeros(input_var.size()).float()\n",
    "        if self.init_rand:\n",
    "            # Experiment with a non-zero starting point...\n",
    "            modifier = torch.normal(means=modifier, std=0.001)\n",
    "        if self.cuda:\n",
    "            modifier = modifier.cuda()\n",
    "        modifier_var = autograd.Variable(modifier, requires_grad=True)\n",
    "\n",
    "        optimizer = optim.Adam([modifier_var], lr=0.0005)\n",
    "\n",
    "        for search_step in range(self.binary_search_steps):\n",
    "            \n",
    "            #print('Batch: {0:>3}, search step: {1}'.format(batch_idx, search_step))\n",
    "            if self.debug:\n",
    "                #print('Const:')\n",
    "                for i, x in enumerate(scale_const):\n",
    "                    print(i, x)\n",
    "            best_l2 = [1e10] * batch_size\n",
    "            best_score = [-1] * batch_size\n",
    "\n",
    "            # The last iteration (if we run many steps) repeat the search once.\n",
    "            if self.repeat and search_step == self.binary_search_steps - 1:\n",
    "                scale_const = upper_bound\n",
    "\n",
    "            scale_const_tensor = torch.from_numpy(scale_const).float()\n",
    "            if self.cuda:\n",
    "                scale_const_tensor = scale_const_tensor.cuda()\n",
    "            scale_const_var = autograd.Variable(scale_const_tensor, requires_grad=False)\n",
    "\n",
    "            prev_loss = 1e6\n",
    "            for step in range(self.max_steps):\n",
    "                # perform the attack\n",
    "                loss, dist, output, output_exp, adv_img = self._optimize(\n",
    "                    optimizer,\n",
    "                    model,\n",
    "                    exp_model,\n",
    "                    intr_model,\n",
    "                    input_var,\n",
    "                    modifier_var,\n",
    "                    target,\n",
    "                    target_var,\n",
    "                    target_var_exp,\n",
    "                    scale_const_var,\n",
    "                    gradcam,\n",
    "                    input_orig)\n",
    "\n",
    "                if step % 100 == 0 or step == self.max_steps - 1:\n",
    "                    print('Step: {0:>4}, loss: {1:6.4f}, dist: {2:8.5f}, modifier mean: {3:.5e}'.format(step, loss, dist.mean(), modifier_var.data.mean()))\n",
    "\n",
    "                if self.abort_early and step % (self.max_steps // 10) == 0:\n",
    "                    if loss > prev_loss * .9999:\n",
    "                        print('Aborting early...')\n",
    "                        break\n",
    "                    prev_loss = loss\n",
    "\n",
    "                # update best result found\n",
    "                for i in range(batch_size):\n",
    "                    target_label = target[i]\n",
    "                    target_exp_label = 0\n",
    "                    output_logits = output[i]\n",
    "                    output_exp_logits = output_exp[i]\n",
    "                    output_label = np.argmax(output_logits)\n",
    "                    output_exp_label = np.argmax(output_exp_logits)\n",
    "                    \n",
    "                    di = dist[i]\n",
    "                    if self.debug:\n",
    "                        if step % 100 == 0:\n",
    "                            print('{0:>2} dist: {1:.5f}, output: {2:>3}, {3:5.3}, target {4:>3}'.format(\n",
    "                                i, di, output_label, output_logits[output_label], target_label))\n",
    "                    \n",
    "                    if di < best_l2[i] and self._compare(output_logits, target_label) and self._compare(output_exp_logits, target_exp_label):\n",
    "                        if self.debug:\n",
    "                            print('{0:>2} best step,  prev dist: {1:.5f}, new dist: {2:.5f}'.format(\n",
    "                                  i, best_l2[i], di))\n",
    "                        best_l2[i] = di\n",
    "                        best_score[i] = output_label\n",
    "                    \n",
    "                    if di < o_best_l2[i] and self._compare(output_logits, target_label) and self._compare(output_exp_logits, target_exp_label):\n",
    "                        if self.debug:\n",
    "                            print('{0:>2} best total, prev dist: {1:.5f}, new dist: {2:.5f}'.format(\n",
    "                                  i, o_best_l2[i], di))\n",
    "                        o_best_l2[i] = di\n",
    "                        o_best_score[i] = output_label\n",
    "                        o_best_attack[i] = adv_img[i]\n",
    "                    \n",
    "                    #print('o_best_l2: ', o_best_l2[i])\n",
    "                    cond1 = di < o_best_l2[i]\n",
    "                    cond2 = self._compare(output_logits, target_label)\n",
    "                    cond3 = self._compare(output_exp_logits, target_exp_label)\n",
    "                    #print('cond1: {} and cond2:{} and cond3: {}'.format(cond1, cond2, cond3))\n",
    "                \n",
    "                sys.stdout.flush()\n",
    "                # end inner step loop\n",
    "\n",
    "            # adjust the constants\n",
    "            batch_failure = 0\n",
    "            batch_success = 0\n",
    "            for i in range(batch_size):\n",
    "                if self._compare(best_score[i], target[i]) and best_score[i] != -1:\n",
    "                    # successful, do binary search and divide const by two\n",
    "                    upper_bound[i] = min(upper_bound[i], scale_const[i])\n",
    "                    if upper_bound[i] < 1e9:\n",
    "                        scale_const[i] = (lower_bound[i] + upper_bound[i]) / 2\n",
    "                    if self.debug:\n",
    "                        print('{0:>2} successful attack, lowering const to {1:.3f}'.format(\n",
    "                            i, scale_const[i]))\n",
    "                else:\n",
    "                    # failure, multiply by 10 if no solution found\n",
    "                    # or do binary search with the known upper bound\n",
    "                    lower_bound[i] = max(lower_bound[i], scale_const[i])\n",
    "                    if upper_bound[i] < 1e9:\n",
    "                        scale_const[i] = (lower_bound[i] + upper_bound[i]) / 2\n",
    "                    else:\n",
    "                        scale_const[i] *= 10\n",
    "                    if self.debug:\n",
    "                        print('{0:>2} failed attack, raising const to {1:.3f}'.format(\n",
    "                            i, scale_const[i]))\n",
    "                if self._compare(o_best_score[i], target[i]) and o_best_score[i] != -1:\n",
    "                    batch_success += 1\n",
    "                else:\n",
    "                    batch_failure += 1\n",
    "\n",
    "            print('Num failures: {0:2d}, num successes: {1:2d}\\n'.format(batch_failure, batch_success))\n",
    "            sys.stdout.flush()\n",
    "            # end outer search loop\n",
    "            \n",
    "            # added by raj\n",
    "            if batch_success == batch_size:\n",
    "                break\n",
    "                \n",
    "        return o_best_attack, o_best_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TARGETED = True\n",
    "MAX_STEPS = 1000\n",
    "SEARCH_STEPS = 6\n",
    "NO_CUDA = False\n",
    "DEBUG = False\n",
    "\n",
    "attack = AttackCarliniWagnerL2(\n",
    "        targeted=TARGETED,\n",
    "        max_steps=MAX_STEPS,\n",
    "        search_steps=SEARCH_STEPS,\n",
    "        cuda=not NO_CUDA,\n",
    "        debug=DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=800, out_features=500, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=500, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = D.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
    "test_loader = D.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
    "\n",
    "\n",
    "# Define what device we are using\n",
    "print(\"CUDA Available: \",torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# # Initialize the network\n",
    "pretrained_model = \"model/fmnist/v2/fmnist_cnn.pt\"\n",
    "# Initialize the network\n",
    "model = Net(model_layers, num_classes=NUM_CLASSES).cuda()\n",
    "\n",
    "# Load the pretrained model\n",
    "model.load_state_dict(torch.load(pretrained_model))\n",
    "\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()\n",
    "\n",
    "##########################################\n",
    "\n",
    "\n",
    "# # Initialize the explanation model\n",
    "NUM_CLASSES_EXP = 2 \n",
    "class_ind = 6\n",
    "model_dir = 'data/defender/fmnist/exp_model_data/for_target/' + str(class_ind) + '/model'\n",
    "pretrained_exp_model = model_dir+\"/exp_model.pt\"\n",
    "\n",
    "exp_model = Net(model_layers, num_classes=NUM_CLASSES_EXP).cuda()\n",
    "\n",
    "# Load the pretrained model\n",
    "exp_model.load_state_dict(torch.load(pretrained_exp_model))\n",
    "\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "exp_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rough code starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for batch_idx, (input_tensor, input_label) in enumerate(train_loader):\n",
    "    # clean image\n",
    "    input_tensor = input_tensor.float().cuda()\n",
    "    \n",
    "    # original label for the clean image\n",
    "    input_label = input_label.cuda() ## input_label.item() will give you the scalar label\n",
    "    \n",
    "    if input_label!=6:\n",
    "        continue\n",
    "    else:\n",
    "        count +=1\n",
    "        \n",
    "        if count == 1:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input_tensor = input_tensor.transpose(1,3).transpose(1,2)\n",
    "input_tensor = nn.Upsample(scale_factor=8, mode='bilinear')(input_tensor)\n",
    "input_tensor= input_tensor.to(device)\n",
    "output_intr = intr_model(input_tensor) # shape : torch.Size([1, 1, 224, 224])\n",
    "ave_p = nn.AvgPool2d(kernel_size=8, stride=None, padding=0)\n",
    "input_intr_tensor = ave_p(output_intr) #[1,1,28,28]\n",
    "input_intr_tensor = (input_intr_tensor - torch.min(input_intr_tensor))/(torch.max(input_intr_tensor) - torch.min(input_intr_tensor))\n",
    "# obtain prediction\n",
    "output_exp = exp_model(input_intr_tensor)\n",
    "index = np.argmax(output_exp.cpu().data.numpy())\n",
    "index_prob = torch.nn.functional.softmax(output_exp)[0][index]\n",
    "print('Classification of the explanation is class: {} with probability:{}'.format(index, index_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_intr_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = input_intr_tensor.cpu().detach().numpy().astype(dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xn = (x-np.min(x))/(np.max(x) - np.min(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(xn[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#a,b = iter(train_loader).next()\n",
    "a = input_tensor.cpu()\n",
    "an = a.numpy().astype(dtype='float32')\n",
    "an = an.transpose(0,2,3,1)\n",
    "at = torch.from_numpy(an.transpose(0,3,1,2))\n",
    "at = nn.Upsample(scale_factor=8, mode='bilinear')(at)\n",
    "at = at.to(device)\n",
    "output_intr = intr_model(at)\n",
    "# output_intr.shape # torch.Size([1, 1, 224, 224])\n",
    "\n",
    "ave_p = nn.AvgPool2d(kernel_size=8, stride=None, padding=0)\n",
    "output_intr_resized = ave_p(output_intr)\n",
    "\n",
    "# on = output_intr.cpu().detach().numpy()\n",
    "# onr = resize(on, (1,1,28,28)) #(1, 1, 28, 28) # try average pooling instead\n",
    "\n",
    "input_intr_tensor = output_intr_resized\n",
    "# obtain prediction\n",
    "output_exp = exp_model(input_intr_tensor)\n",
    "index = np.argmax(output_exp.cpu().data.numpy())\n",
    "index_prob = torch.nn.functional.softmax(output_exp)[0][index]\n",
    "print('Classification of the explanation is class: {} with probability:{}'.format(index, index_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "desired_source_label = 5\n",
    "input_tensors = []\n",
    "input_labels = []\n",
    "for batch_idx, (input_tensor, input_label) in enumerate(test_loader):\n",
    "    for i, label in enumerate(input_label):\n",
    "        if label == desired_source_label:\n",
    "            input_tensors.append(input_tensor[i].cpu().numpy())\n",
    "            \n",
    "input_tensors = torch.from_numpy(np.array(input_tensors))           \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# rough code ends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------iteration: 0----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/user/raj_vardhan/.conda/envs/tf-gpu-env/lib/python3.6/site-packages/torch/nn/functional.py:2479: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "/scratch/user/raj_vardhan/.conda/envs/tf-gpu-env/lib/python3.6/site-packages/torch/nn/functional.py:2390: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:    0, loss: 13.6347, dist:  0.00000, modifier mean: -1.30002e-04\n",
      "Step:  100, loss: 9.7847, dist:  0.00745, modifier mean: -8.92916e-04\n",
      "Step:  200, loss: 9.7659, dist:  0.00745, modifier mean: -1.10711e-03\n",
      "Step:  300, loss: 9.7685, dist:  0.00791, modifier mean: -1.22342e-03\n",
      "Aborting early...\n",
      "Num failures: 20, num successes:  0\n",
      "\n",
      "Step:    0, loss: 96.2582, dist:  0.00792, modifier mean: -1.25945e-03\n",
      "Step:  100, loss: 87.6361, dist:  0.35959, modifier mean: -6.28686e-03\n",
      "Step:  200, loss: 86.7719, dist:  0.43840, modifier mean: -7.92941e-03\n",
      "Step:  300, loss: 86.4651, dist:  0.47101, modifier mean: -8.85225e-03\n",
      "Step:  400, loss: 86.3037, dist:  0.48951, modifier mean: -9.46267e-03\n",
      "Step:  500, loss: 86.2076, dist:  0.50124, modifier mean: -9.93562e-03\n",
      "Step:  600, loss: 86.1494, dist:  0.50958, modifier mean: -1.03259e-02\n",
      "Step:  700, loss: 86.1410, dist:  0.51425, modifier mean: -1.06477e-02\n",
      "Aborting early...\n",
      "Num failures: 20, num successes:  0\n",
      "\n",
      "Step:    0, loss: 768.8056, dist:  0.51438, modifier mean: -1.07050e-02\n",
      "Step:  100, loss: 478.1537, dist:  5.97565, modifier mean: -2.56477e-02\n",
      "Step:  200, loss: 403.7257, dist:  8.62769, modifier mean: -3.17160e-02\n",
      "Step:  300, loss: 362.4434, dist: 10.39244, modifier mean: -3.51117e-02\n",
      "Step:  400, loss: 341.2538, dist: 11.20933, modifier mean: -3.69518e-02\n",
      "Step:  500, loss: 328.5259, dist: 11.76012, modifier mean: -3.83692e-02\n",
      "Step:  600, loss: 316.7331, dist: 12.41595, modifier mean: -3.95367e-02\n",
      "Step:  700, loss: 308.4167, dist: 12.86419, modifier mean: -4.05203e-02\n",
      "Step:  800, loss: 303.9831, dist: 13.08708, modifier mean: -4.12224e-02\n",
      "Step:  900, loss: 301.0665, dist: 13.22197, modifier mean: -4.13407e-02\n",
      "Step:  999, loss: 299.0966, dist: 13.31886, modifier mean: -4.15808e-02\n",
      "Num failures:  3, num successes: 17\n",
      "\n",
      "Step:    0, loss: 592.2180, dist: 13.32113, modifier mean: -4.16046e-02\n",
      "Step:  100, loss: 304.7971, dist: 14.81668, modifier mean: -4.30917e-02\n",
      "Step:  200, loss: 299.5919, dist: 14.30635, modifier mean: -4.23612e-02\n",
      "Step:  300, loss: 296.8726, dist: 14.02005, modifier mean: -4.19330e-02\n",
      "Step:  400, loss: 294.9366, dist: 13.81090, modifier mean: -4.16585e-02\n",
      "Step:  500, loss: 293.6440, dist: 13.67119, modifier mean: -4.14551e-02\n",
      "Step:  600, loss: 292.5136, dist: 13.55640, modifier mean: -4.13222e-02\n",
      "Step:  700, loss: 291.9637, dist: 13.49481, modifier mean: -4.22993e-02\n",
      "Step:  800, loss: 290.8997, dist: 13.41508, modifier mean: -4.15069e-02\n",
      "Step:  900, loss: 290.2366, dist: 13.36196, modifier mean: -4.12574e-02\n",
      "Step:  999, loss: 289.7307, dist: 13.32465, modifier mean: -4.11479e-02\n",
      "Num failures:  0, num successes: 20\n",
      "\n",
      "succ:20 fail:0 mean L2 of succ attacks:14.769693374633789\n",
      "\n",
      "\n",
      "-------iteration: 1----------\n",
      "Step:    0, loss: 9.3116, dist:  0.00000, modifier mean: -2.33982e-04\n",
      "Step:  100, loss: 7.4179, dist:  0.00899, modifier mean: -5.32631e-04\n",
      "Step:  200, loss: 7.4160, dist:  0.00961, modifier mean: -5.84027e-04\n",
      "Step:  300, loss: 7.4040, dist:  0.00929, modifier mean: -6.51847e-04\n",
      "Step:  400, loss: 7.4164, dist:  0.01000, modifier mean: -6.76019e-04\n",
      "Aborting early...\n",
      "Num failures: 20, num successes:  0\n",
      "\n",
      "Step:    0, loss: 72.3609, dist:  0.00999, modifier mean: -7.02743e-04\n",
      "Step:  100, loss: 62.0928, dist:  0.45258, modifier mean: -4.80870e-03\n",
      "Step:  200, loss: 61.6723, dist:  0.49960, modifier mean: -5.50852e-03\n",
      "Step:  300, loss: 61.5244, dist:  0.52004, modifier mean: -5.87971e-03\n",
      "Step:  400, loss: 61.4861, dist:  0.53040, modifier mean: -6.04429e-03\n",
      "Step:  500, loss: 61.4375, dist:  0.53427, modifier mean: -6.21420e-03\n",
      "Step:  600, loss: 61.4118, dist:  0.53732, modifier mean: -6.33678e-03\n",
      "Step:  700, loss: 61.3957, dist:  0.53920, modifier mean: -6.44561e-03\n",
      "Step:  800, loss: 61.3881, dist:  0.54051, modifier mean: -6.53051e-03\n",
      "Step:  900, loss: 61.3793, dist:  0.54124, modifier mean: -6.62147e-03\n",
      "Step:  999, loss: 61.3781, dist:  0.54135, modifier mean: -6.70030e-03\n",
      "Num failures: 20, num successes:  0\n",
      "\n",
      "Step:    0, loss: 516.3063, dist:  0.54144, modifier mean: -6.79177e-03\n"
     ]
    }
   ],
   "source": [
    "loader = train_loader\n",
    "\n",
    "iterations = 0\n",
    "thresh_on_iterations = 3\n",
    "\n",
    "success_record = []\n",
    "\n",
    "gradcam = GradCam(model, target_layer_names=[\"4\"])\n",
    "\n",
    "best_l2s = []\n",
    "\n",
    "#for batch_idx, (input_tensor, input_label) in enumerate(loader):\n",
    "input_label = torch.from_numpy(np.array([desired_source_label]*BATCH_SIZE))\n",
    "i = 30\n",
    "while i < 100:\n",
    "    input_tensor = input_tensors[i:i+BATCH_SIZE]\n",
    "    \n",
    "    print('\\n\\n-------iteration: {}----------'.format(iterations))\n",
    "    \n",
    "    # clean image\n",
    "    input_tensor = input_tensor.float().cuda()\n",
    "    \n",
    "    # original label for the clean image\n",
    "    input_label = input_label.cuda() ## input_label.item() will give you the scalar label\n",
    "    \n",
    "#     if input_label.item() == 6:\n",
    "#         continue\n",
    "    \n",
    "#     if input_label!=5:\n",
    "#         continue\n",
    "    \n",
    "    pred_input = model(input_tensor)\n",
    "    pred_prob_input = F.softmax(pred_input, dim=1)\n",
    "#     print('prediction of clean sample: {} with probability: {}'\n",
    "#           .format(torch.argmax(pred_prob_input),torch.max(pred_prob_input)))\n",
    "    \n",
    "    # if the attack is targeted, we will target the next class modulo number of classes\n",
    "    if TARGETED == True:\n",
    "        target = (input_label+1)%10\n",
    "    # else the target is kept as the original label as per attack design\n",
    "    else:\n",
    "        target = input_label\n",
    "    \n",
    "    # result obtained is a numpy array\n",
    "    adversarial_img, best_l2 = attack.run(model, exp_model, intr_model, input_tensor, target, gradcam, batch_idx)\n",
    "    \n",
    "    # reshape\n",
    "    adversarial_img = np.transpose(adversarial_img, (0,3,1,2))\n",
    "    \n",
    "    #adv_imgs.append(adversarial_img)\n",
    "    best_l2s.extend(best_l2)\n",
    "    \n",
    "    # conver to torch tensor\n",
    "    adversarial_tensor = torch.from_numpy(adversarial_img).cuda()\n",
    "    \n",
    "    # obtain the prediction by the model\n",
    "    pred_adv = model(adversarial_tensor)\n",
    "    pred_prob_adv = F.softmax(pred_adv, dim=1)\n",
    "    pred_class_adv = torch.argmax(pred_prob_adv, dim=1)\n",
    "#     print('prediction of adversarial sample: {} with probability: {}'.\n",
    "#       format(torch.argmax(pred_prob_adv),torch.max(pred_prob_adv)))\n",
    "\n",
    "    \n",
    "#     clean_label = input_label.item()\n",
    "#     #pred_adv_label = torch.max(pred_adv, 1)[1].item()\n",
    "#     pred_adv_label = torch.argmax(pred_prob_adv)\n",
    "    \n",
    "#     print('Original label of the clean image is: {} and predicted label of the adversarial image is {}'\\\n",
    "#           .format(clean_label,pred_adv_label))\n",
    "    \n",
    "    if TARGETED:\n",
    "        result = pred_class_adv == target\n",
    "    else:\n",
    "        result = pred_class_adv != input_label\n",
    "    \n",
    "    success_record.extend(result.cpu().numpy())\n",
    "    \n",
    "    # log results\n",
    "    success_record_npy = np.array(success_record)\n",
    "    best_l2s_npy = np.array(best_l2s)\n",
    "    ind_succ = np.where(success_record_npy==True)[0]\n",
    "    succ_l2s = best_l2s_npy[ind_succ]\n",
    "    print('succ:{} fail:{} mean L2 of succ attacks:{}'.format(succ_l2s.shape[0], success_record_npy.shape[0] - succ_l2s.shape[0], np.mean(succ_l2s)))\n",
    "    \n",
    "    iterations += 1\n",
    "    i += BATCH_SIZE\n",
    "    if iterations == thresh_on_iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.084463"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(best_l2s[0:10])\n",
    "#8.084463,7.936684608459473,14.769693374633789"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-31af925551e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8.084463\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7.936684608459473\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14.769693374633789\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.mean([8.084463,7.936684608459473,14.769693374633789])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('results/adaptive/success_record.npy',success_record)\n",
    "np.save('results/adaptive/best_l2s.npy',best_l2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3.3805873], [11.200596], [11.817215], [8.766975], [4.6256895], [8.903587]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_l2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_clean = input_tensor.cpu().numpy()\n",
    "plt.imshow(img_clean[0,0],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3ca0957f98>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(adv_imgs[1][0,0],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_tensor, input_label = iter(loader).next()\n",
    "# clean image\n",
    "input_tensor = input_tensor.cuda()\n",
    "\n",
    "# original label for the clean image\n",
    "input_label = input_label.cuda() ## input_label.item() will give you the scalar label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = (input_label+1)%10\n",
    "target_onehot = torch.zeros(target.size() + (2,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_onehot = target_onehot.cuda()\n",
    "target_onehot.scatter_(1, target.unsqueeze(1), 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_exp = torch.tensor([0]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x+2\n",
    "z = y*y*3\n",
    "out = z.mean()\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
